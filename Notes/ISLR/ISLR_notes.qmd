---
title: "ISLR Notes"
format: 
  html:
    toc: true
    toc-location: left 
    toc-title: Chapters
    html-math-method: katex
    number-sections: true
editor: visual
code-overflow: wrap
---

## Libraries

```{r, message=F, warning=F}
library(glmnet)
library(tidyverse)
```

## Chapter 2: Statistical Learning

### Figure 2.1

While reading the main textbook, it will be helpful to make a list of flashcards of basic terms and definitions that are *italicized* in the chapters. The first key set of terms are input variables and output variables. *Input variables* go by lots of names, but the main commonality is that input variables go on the right hand side of an equation.[^1] *Output variables* are the outputs of the function we seek to learn using the input variables.

[^1]: Strictly speaking, input variables are arguments to the fixed but unknown function *f* that we are trying to learn.

```{r, message = F}
## Anything that follows a # inside a R code block is a comment 
## We use them to provide notes to ourselves or others 
## about what a line or lines of code do to help solve 
## the problem we are working on. 
## I personally find ## easier to read than #. 
## Most of my comments here relate to where you find certain functions
## in the set of tidyverse packages. 
## Learning what you want to do is as important 
## as learning where functions are located. 

## readr::read_csv()
ads = read_csv("https://www.statlearning.com/s/Advertising.csv",
               show_col_types = FALSE) |>
  ## Remove the row names 
  ## dplyr::select()
  select(-`...1`)

ads |>
  ## tidyr::pivot_longer()
  pivot_longer(-sales, 
               names_to = "type", 
               values_to = "spend") |>
  ## dplyr::mutate, dplyr::case_when(), stringr::str_detect, stringr::str_to_title()
  mutate(type = case_when(
    str_detect(type, "TV")~"TV",
    TRUE~str_to_title(type)
  )) |>
  ## ggplot2 for all functions unless noted
  ggplot(aes(spend, 
             sales, 
             group = type))+
  geom_point() +
  ## forcats::fct_rev()
  facet_wrap(~fct_rev(type), 
             scales = "free", 
             strip.position = "bottom")+
  theme_bw()+
  geom_smooth(method = "lm", 
              se = F)+
  theme(strip.background = element_blank(),
        strip.placement = "outside")
```

The base version of this figure is slightly more annoying, but that's just my opinion. You may prefer it.

```{r, message = F, warning = F}
ads = read.csv("https://www.statlearning.com/s/Advertising.csv",
               stringsAsFactors = FALSE) 
ads = ads[,-1]

ads_long = ads |>
  reshape(direction = "long", 
          v.names = "spend", 
          timevar ="type", 
          times = names(ads[c(1:3)]),
          varying = c(1:3))

ads_long$type = ifelse(ads_long$type != "TV", 
                       tools::toTitleCase(ads_long$type), 
                       ads_long$type)

ads_long |>
  ## ggplot2 for all functions unless noted
  ggplot(aes(spend, 
             sales, 
             group = type))+
  geom_point() +
  ## forcats::fct_rev()
  facet_wrap(~fct_rev(type), 
             scales = "free", 
             strip.position = "bottom")+
  theme_bw()+
  geom_smooth(method = "lm", 
              se = F)+
  theme(strip.background = element_blank(),
        strip.placement = "outside")
```

### Figure 2.2

```{r, message = F}
income = read_csv("https://www.statlearning.com/s/Income1.csv", 
                  show_col_types = FALSE) |>
  ## Another way to remove the first column 
  ## tidyselect::contains()
  select(-contains("."))

income |>
  ggplot(aes(Education, Income))+
  geom_point()+
  ylab("Income")+
  xlab("Years of Education")+
  theme_bw()
             
```

We can think of the fundamental learning problem of interest that of learning some function that relates our output and input variables.

$$
\begin{aligned}
Y = f(X) + \epsilon
\end{aligned}
$$

Y corresponds to the output variable and $X$ is a vector of input variables. The $\epsilon$ (epsilon) is the random error term, for which we may or may not make an assumption about. Usually when we make assumptions it is that it is independent of $X$ and (trivially because we can always normalize a set of values) has mean zero.

### Why Estimate $f$?

#### Prediction

To be frank (and opinionated), prediction is the reason why we most of the time people are interested in machine learning. We have a set of input variables, and we use them to estimate our function of interest. Over the course of the semester, we will learn some different ways to estimate this function but the most common estimation strategy is some form of regression.

We determine if our prediction is any good by some metric. A common one might be accuracy. How often does our prediction get sufficiently close to our observed output variables. We break this up into *reducible* and *irreducible* errors. Reducible error is something we can make smaller, often by collecting more or better data. Irreducible error is something we are stuck with because it is a function of a variable we can never collect data about (that pesky $\epsilon$).

We can decompose the expected error into reducible and irreducible parts.

$$
\begin{aligned}
E(Y-\hat{Y})^2 &= E[f(X) + \epsilon - \hat{f}(X)]^2 \\
E(Y-\hat{Y})^2 &= E[f(X) + \epsilon - \hat{f}(X)]E[f(X) + \epsilon - \hat{f}(X)] \\
E(Y-\hat{Y})^2 &= E[f(X)^2 + 2f(X)\epsilon - 2f(X)\hat{f}(X) + \epsilon^2 - 2\epsilon\hat{f}(X) + \hat{f}(X)^2] \\
E(Y-\hat{Y})^2 &= E[f(X)^2 - 2f(X)\hat{f}(X) + \hat{f}(X)^2] + E[\epsilon^2] \\
E(Y-\hat{Y})^2 &= E[(f(X)^2 - \hat{f}(X))^2] + V[\epsilon] \\
E(Y-\hat{Y})^2 &= [f(X)^2 - \hat{f}(X)]^2 + V[\epsilon]
\end{aligned}
$$

How did we do that?

-   The second line expands out the squared term.

-   The third line applies the identity that $(x+y-z)^2 = x^2 + 2xy - 2xz + y^2 -2yz + z^2$.

-   The fourth line does a couple of steps at once based on properties of the expectation operator: $E[c] = c, E[x + y] = E[x] + E[y]$ and then groups terms.

-   The fifth line substitutes in the appropriate definition of the variance and condenses the square. The sixth line applies the expectation of a constant property again.

Irreducible error matters because the irreducible error bounds the accuracy of our predictions of Y, and we never actually know what it is in practice.

#### Inference

The book notes that another reason we care about the relationship between input and output variables is to learn something about their association. Note here the word "association" which does mean related but does not imply that the input variables cause the output variables. That is a separate problem of causal inference. People are sloppy all the time about this, but for the purpose of learning you should always be clear about when the inference you are drawing is causal.[^2]

[^2]: Whether non-causal inferences are interesting is a separate question.

### How do we estimate $f$?

Suppose we observe a set of *n* different data points. We call these observations *training data* because we use them to train our estimation method how to estimate *f* our function of interest. In the book's notation:

$$
\begin{aligned}
\{(x_1, y_1),...,(x_n, y_n)\}, x_i = (x_{i1}, ...x_{ip})^T
\end{aligned}
$$

That compact expression looks confusing if we have not seen them before, but all it is seeing is that we have a set of realizations of input variables $X$ and output variable $y$ and each $X$ is the realization of of the vector of input variables. Here's an example in code.

```{r}
## number of training data pairs
set.seed(123)
n = 5

## tibble::tibble()
training_data = tibble(
  x1 = rnorm(n),
  x2 = rnorm(n),
  x3 = rnorm(n),
  y = x1 + x2 + x3 + runif(n)
) |>
  ## dplyr::relocate()
  relocate(y, .before = x1)

training_data

```

```{r}
## Base version 
n = 5 
set.seed(123)
training_data = data.frame(
  x1 = rnorm(n),
  x2 = rnorm(n),
  x3 = rnorm(n)
)
training_data[["y"]] = training_data$x1 + training_data$x2 + training_data$x3 + runif(n)
training_data = training_data[union("y", names(training_data))]

training_data 
```

The other part of that equation that might be strange to you is the $T$ at the end. This is the symbol for transpose, and is done because by convention we think of all of the operations as column operations.

#### Parametric Methods

Parametric means that we make some assumptions about the parameters of the function. For example, we might assume that the function is linear in parameters, or that no value can ever be above 42, or a host of other possible assumptions.

In parametric modeling, we first make an assumption about the functional form of the $f$ and then follow some procedure to estimate a model of that functional form.

What's the benefit? Well since we have assumed that the problem must look a certain way, estimating the function is now much easier. The downside (and there is always a downside in everything) is that if we are wrong about this assumption we will be very wrong. Some parametric assumptions (linear function) tend to be used a lot.

#### Non-parametric Methods

Non-parametric methods do not make explicit assumptions about the functional form of the function of interest. Instead, they simply seek to estimate it in a way that flexibly fits the data without fitting *too well*. We will see what that means as we move through the book, but the word to get familiar with is *overfitting*.

Note that for both parametric and non-parametric methods the reason why we care about overfitting is that we are seeking to learn about some function $f$ based on our training data, not what the best functional fit of the data we have is. Overfitting only matters because we do not care about the training data for its own sake, but rather what it can tell us about future observations.

### Supervised and Unsupervised Learning

Supervised learning implies that we have some observations for which we know the outputs. It is "supervised" because we are telling our machine assistant what is the correct answer. The vast majority of prediction problems are supervised learning problems. Unsupervised learning occurs when we do not have a correct output variable to predict, but still seek to understand relationships between variables or between observations. Unsupervised topic models would be an example of the latter in text analysis.

### Regression vs. Classification

Because methods come from different disciplines, there is some confusing vocabulary distinctions. For the purpose of this book, when you hear a regression problem, you should think of trying to predict a continuous or quantitative output variable. Is more advertising associated with increased sales is a regression problem. In contrast a classification problem deals with ordinal or nominal data where the goal is the predict what bucket a set of observations might fall into. Will a person with a set of attributes be likely to get into Cal is a classification problem.[^3]

[^3]: This distinction is annoying to someone who comes at these questions from a different discipline. Much of the machine learning literature's hodgepodge of terms tells you more about the authors' educational background as it does the problem at hand.

### Assessing Model Accuracy

There are an infinite number of possible models that we can use to estimate our function of interest given our input variables and output variables. Therefore we need some way to determine which model is "best" and so also need some metric to define "best." A common one in the regression setting is Mean Squared Error (MSE).

$$
\begin{aligned}
MSE = \frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(x_i))^2
\end{aligned}
$$

Mean Squared Error is an example of a "loss function." By its name, you might infer correctly that the goal is to pick the model that has the smallest loss function of interest. Examine the equation for MSE to convince yourself that the MSE will be as low as possible if the prediction that our estimate gives is identical to the true value of our observed output variable.

As stated earlier, we do not care about the MSE of the training data per se. We care about the MSE of our learning method on unseen data, which is often referred to as *test* data. Supposing that we have a large number of training observation to estimate a model, we simply sub in new data pairs into the above equation to get the test MSE.

One key point regarding flexibility of models is that as the flexibility of our model increases (the fewer assumptions we put on functional form) we will observe a decrease in the training MSE and a U shape in the test MSE. *Overfitting* occurs when we have a low training MSE but a high test MSE.

### Bias-Variance Tradeoff

The most important trade-off in all of machine learning is the Bias-Variance tradeoff. Bias refers to (loosely) how off our estimate of the true function would be as the number of training examples gets very large. Formally the bias of some statistic $\theta$ is \$B\[\\theta\] = E\[\\theta - \\hat{\\theta}\]\$. We can think of bias as the error introduced by approximating the problem of interest. Variance on the other hand refers to how much our estimate $\hat{f}$ would change if we had used a different training data set. High variance means that a small change in the training data leads to a large change in $\hat{f}$.

The book gives the following equation for the decomposition of the MSE into the variance of our estimator, the squared bias of our estimator and the error term.[^4]

[^4]: The derivation is boring. Recall the decomposition of error and add and subtract $E[\hat{f}]$ from the right hand side. Group the terms together and substitute in appropriate named quantitites.

    $$
    \begin{aligned}
    E[Y-\hat{Y}]^2 &= E[(f- \hat{f})^2] + V[\epsilon] \\
    E[Y-\hat{Y}]^2 &= E[f + E[\hat{f}] - E[\hat{f}] - \hat{f}] + V[\epsilon] \\
    E[Y-\hat{Y}]^2 &= E[\hat{f} - E[\hat{f}]]^2 + [f - E[\hat{f}]]^2 + V[\epsilon] \\
    E[Y-\hat{Y}]^2 &= V[\hat{f}] + B[\hat{f}]^2 + V[\epsilon]
    \end{aligned}
    $$

$$
\begin{aligned}
E[y_0 - \hat{f}(x_0)]^2 = V[\hat{f}(x_0)] + [B(\hat{f}(x_0))]^2 + V[\epsilon]
\end{aligned}
$$

What does this equation tell us about the expected MSE on the test set. First, notice that every quantity is non-negative. Second, notice that irreducible error is part of the equation and serves as the upper bound because even if we had an estimate that had no variance and no bias, we would still have $V[\epsilon]$ left over.

The Bias-Variance tradeoff tells us that good test set performance has low bias and low variance. The tradeoff is that we can always draw some curve to the data at hand that perfectly captures every data point, but if we take that method to a different set of data points we will have increased variance. Conversely, we can always make variance 0 by fitting a horizontal line through data, but this will be unlikely to capture the true fit of the function increasing bias.[^5]

[^5]: In causal inference, we tend to care a lot more about the bias of our estimator than the variance.

#### Classification Loss Functions

Accuracy in the classification setting is conceptually similar with appropriate changes for the fact that we are now interested in the *training error rate*, the proportion of mistakes if we apply our learned estimate to the training observations.

$$
\begin{aligned}
Err = \frac{1}{n}\sum_{i=1}^nI(y_i \neq \hat{y}_i)
\end{aligned}
$$

The lowest test error occurs via the *Bayes Classifier* a method that assigns each observation to the most likely class given predictor values.

$$
\begin{aligned}
BC = Pr(Y=j|X = x_0)
\end{aligned}
$$

and the overall error rate will be $1 - E[\max_{\j} Pr(Y=j|X))$, analogous to irreducible error in the classification case.

### Lab

The lab at the end of Chapter 2 just reproduces some basic R functions.

```{r}
## Make a vector 
x = c(1,6,2)
y = c(1,4,3)
## Element wise addition
x + y 
```

```{r}
## Make a matrix 
x = matrix(data = c(1,2,3,4), 
           nrow = 2, 
           ncol = 2, 
           byrow = TRUE)
x
sqrt(x)

```

```{r}
## Generate some random normal realizations 
set.seed(123)
x = rnorm(500)
y = x + rnorm(50, mean = 50, sd = .1)
cor(x,y)
```

### Indexing Data

I'm choosing to skip the graphics section because this class uses the tidyverse and because base R graphics look ugly to me without extensive customization.

Indexing is incredibly important in any programming. A key note in R is that it is a 1 based index instead of a 0 based index.

```{r}
A = matrix(1:16, 4,4, byrow = T)
A
A[2,3]

## We can also get slices 
A[c(1,3), c(2,4)]
A[1:3, 2:4]
A[1:2,] # Getting just the first two rows 
A[,1:2] # first two columns
A[-c(1,3),] # get only 2 and 4th row
```

### Exercises

#### Problem 7

Here is an example of applying a function to a data set.

```{r}
knn_data = tibble(
  id = 1:6,
  x1 = c(0,2,0,0,-1,1),
  x2 = c(3,0,1,1,0,1),
  x3 = c(0,0,3,2,1,1),
  y = c("red", "red", "red","green","green","red")
)

getEuclideanDistance = function(point = c(0,0,0), vec){
  dist = sqrt((vec[1]- point[1])^2 + (vec[2]-point[2])^2 + (vec[3]-point[3])^2)
  return(dist)
}

getClosestPoints = function(k){
  
}
knn_data |>
  rowwise()|>
  mutate(d = getEuclideanDistance(vec = c(x1, x2, x3)))

```

## Chapter 3: Linear Regression

Linear regression is a simple approach for supervised learning. We might use it to determine if a relationship exists between variables, the strength of the (linear) relationship, associations between multiple variables, or whether there are interaction effects.[^6]

[^6]: The book often refers to this as "synergy"

The relationship we seek to fit is \$\$Y \\approx B_i\\textbf{X}\$\$ where we have a series of input variables in a design matrix $\textbf{X}$ and want to predict an output $Y$.

There are lots of ways to do this in R.

1.  We can do it by hand

```{r}
## We will use the advertising data 
Y = as.matrix(ads[, "sales"])
x = as.matrix(ads[, c("TV")])

## We add an intercept term 
intercept = rep(1, nrow(ads))
X = as.matrix(cbind(intercept, x))

## (X'X)^-1X'y is the OLS estimator for any number 
## of input variables X
## The RHS of this is the way to do matrix multiplication and 
## the inverse and transpose operations in R
betas = solve(t(X) %*% X)%*%t(X)%*%Y
betas

### Calculate residuals
resids = Y - X%*%betas 

### Get degrees of freedom 
df = nrow(X) - ncol(X) - 1

### Calculate residual variance 
resids_v = sum(resids^2)/df 

### Get covariance matrix 
beta_cov = resids_v * solve(t(X)%*%X)

## Takes the square root of the diagonal of the covariance matrix
## to get standard errors 
beta_se = sqrt(diag(beta_cov))

round(data.frame(betas, beta_se),3)
```

2.  We can use the base default `lm()`

```{r}
## broom tidy is a way to get the output of lm into a data frame
## Note that you will need to install the package before using
broom::tidy(lm(sales ~ TV, data = ads))
```

Note that our calculation "by hand" matches the estimate and standard errors of the default function.

3.  We can use (and should!) a function from a package that lets us have better standard error estimation.

```{r}
## fixest is a created package, so must be installed before using
broom::tidy(fixest::feols(sales ~ TV, data = ads))
```

### The Math

The optimization problem is to take a set of data points $\{(x_1, y_1),…(x_n,y_n)\}$ that represent $n$ observation pairs. In the model in the chapter the points are the TV advertising budget and the product sales. We want to fit a model that minimizes the least squares criterion, which is our loss function. This model amounts to minimizing the total error between our model fit and all of the data points.[^7] The errors are defined by the residuals, and the residual sum of squares RSS is $(y_1 - \hat{\beta_0} - ...\hat{\beta_nx_n)^2 + ... (y_n - \hat{\beta_0}x_n)^2}$. In words, we make some guesses about our beta coefficients and then subtract the prediction $\hat{\beta_i}x_i$ from the actual value observed.

[^7]: Note that this sounds an awful lot like finding an average and that is intentional.

It turns out in the simple linear regression case, the minimizes for this equation are:

$$\begin{aligned}
\hat{\beta_0} &= \bar{y} - \hat{\beta_1}\bar{x} \\
\hat{\beta_1} &= \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
\end{aligned}$$

The numerator in the second estimate the covariance between the vectors and the denominator is the variance of the input variables. The values with bars over them represent the sample means.

```{r}
## In code 
beta1 = cov(ads[["TV"]], ads[["sales"]]) / var(ads[["TV"]])

beta0 = mean(ads[["sales"]], na.rm = T) - beta1 * mean(ads[["TV"]], na.rm = T)

rbind(beta0, beta1)
```

To assess uncertainty we compute standard errors. The formulas associated with the two coefficients are:

$$\begin{aligned}
SE_{\hat{\beta_0}} &= \sigma^2[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2}] \\
SE_{\hat{\beta_1}} &= \frac{\sigma^2}{\sum_{i=1}^n(x_i -\bar{x})^2}
\end{aligned}$$

```{r}
## in code 

## How far off are we? 
sigma_squared = var(ads[["sales"]]- beta0 - beta1*ads[["TV"]])

## For readability purposes
numerator = sigma_squared*mean(ads[["TV"]]^2) 
denominator = sum((ads[["TV"]]-mean(ads[["TV"]]))^2)
var_beta0 = numerator / denominator 
se_beta0 = sqrt(var_beta0)


var_beta1 = sigma_squared / denominator
se_beta1 = sqrt(var_beta1)

rbind(se_beta0, se_beta1)
```

If we want a confidence interval, we take the coefficient and multiply it plus/minus some critical value times the standard error.

```{r}
## approximately 95% confidence intervals 
lower_bound = beta1 - 2*se_beta1 
upper_bound = beta1 + 2*se_beta1 

rbind(lower_bound, upper_bound)
```

The t-statistic is defined as the coefficient divided by the standard error

```{r}
## We'll get a slightly different answer here due to floating point operations and the calculation of the generalized variance
beta1 / se_beta1
```

### Multiple Regression

Everything that we have done so far generalizes to the multiple regression case. This is good because if it did not we likely would have made a basic category error.

### A word on interactions

If you have an interaction in your model, you always need to put the separate variables into the model (e.g. the main effects). To add interactions into a model, we simply multiply variables together. Here are two commons ways to do that in R.

```{r}
broom::tidy(lm(sales ~ TV + radio + TV*radio, data = ads))
broom::tidy(lm(sales ~ TV + radio + TV:radio, data = ads))

## Also works in fixest 
broom::tidy(fixest::feols(sales ~ TV + radio + TV:radio, data = ads))
```

Similarly, if we want to include powers, we simply add in the appropriate order.

```{r, message = F}
auto = read_csv("https://www.statlearning.com/s/Auto.csv", na = c("?",NA)) |>
  ## drop missing
  na.omit()

## poly is more flexible, raw = T makes it equivalent to the 
## the next way to do it. poly by default only uses orthogonal
## terms. Raw terms are likely to have collinearity 
broom::tidy(lm(mpg ~ poly(horsepower, 2, raw=T), data = auto))

## alternatively 
broom::tidy(lm(mpg ~ horsepower + I(horsepower^2), data = auto))
```

## Chapter 4: Classification

We keep the same setting as before. We have a set of training observations that we can use as a classifier. We want the classifier to perform well not only on the training data, but also on data it has not seen before.

```{r}
### replicating Figure 4 
default = ISLR2::Default

default |>
  ggplot(aes(balance, income, color = default, shape = default))+
  geom_point()+
  theme_minimal()+
  theme(legend.position = "none")+
  labs(title = "Overall Default Rates",
       x = "Balance", y = "Income")

## Box plots 
default |>
  ggplot(aes(default, balance))+
  geom_boxplot()+
  theme_minimal()

default |>
  ggplot(aes(default, income))+
  geom_boxplot()+
  theme_minimal()
```

### Why Not Linear Regression?

In cases of qualitative response with multiple non-ordinal quantities, the coding of cases will affect the output in ways that we do not want it do. Since there is no natural way to convert a qualitative response with more than two levels, we shouldn't use linear regression in this case.

In the binary case there's really no problem, though the book mentions that you can get predictions greater than 1 or less than 0. Whether this is actually a problem is an open question since you can always just truncate and the linear regression approach will be the same as for linear discriminant analysis approaches.

### Logistic Regression

A logistic regression models the probability that Y belongs to a particular category. A logistic regression is so called because we model the probability with the logistic function.

$$
p(X) = \frac{e^{\textbf{X}\beta }}{1 + e^{\textbf{X}B}}
$$ We fit this model via maximum likelihood. We tend to use the odds for interpretation.

$$
\frac{p(X)}{1 - p(X)} = e^{\textbf{X}\beta}
$$ The odds can take on any value between 0 and infinity. Taking the logs of both sides reduces that equation to

$$
log \left(\frac{p(X)}{1 - p(X)}\right)= \textbf{X}\beta
$$ The interpretation of a $\beta$ coefficient is as follows. Increasing X_i by one unit changes the log odds by $\beta_i$. Because the relationship between $p(X)$ and $X$ is not a straight line, it is not correct to interpret $\beta_i$ as a marginal change in $p(X)$ associated with a one-unit increase in $X_i$.

### Estimating a logistic regression

We use maximum likelihood. We'll consider the simple binary case where we have coefficients $\beta_0$ and $\beta_1$ to estimate.

$$
l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i)\prod_{i':y_{i'}= 0} (1- p(x_i))
$$

In R:

```{r}
logit = glm(default~balance, data = default, family = "binomial")
```

```{r, echo = F, message = F}
knitr::kable(broom::tidy(logit))
```

To get the log-odds

```{r}
## We're passing in the default value of the intercept
## and a balance of $1000 to match the predictions example
betas = logit$coefficients %*% c(1,1000)
odds = 1/(1+exp(-betas))
odds
```

Let's try another example

```{r}
l2 = glm(default ~ student, data = default, family = "binomial")

## default for student 
betas = l2$coefficients %*% c(1,1)
yes = exp(betas)/ (1 + exp(betas))
yes
```

### Multiple Logistic Regression

The generalization works the same way as in the linear regression case.

## Chapter 5: Resampling Methods

### Cross Validation {#sec-cv}

Recall that there is a distinction between training error rate and test error rate. The *training error rate* is the average error that results from using a method on the training data. The *test error rate* the average error from using a learning method to predict the response on the new observation not from the training data.

#### Validation Set Approach

Suppose we want to estimate the test error associated with fitting a particular learning method on a set of observations. In the *Validation Set* approach, we randomly divide the available observations into a training set and a validation set. We fit the model on the training data, and the fitted model is used to predict the responses for the validation set.

```{r}
set.seed(1)
auto = ISLR2::Auto

## Randomly split the 392 observations into two 
## sets of 196 observations 
idx = sample(392, 196, replace = F)
train = auto[idx,]
test = auto[-idx,]

model = lm(mpg ~ horsepower, data = train)
preds = predict(model, test)
mse_pred = mean((auto$mpg[-idx] - preds)^2)
```

There are some drawbacks with just using a single validation set.

1.  Validation estimates of the test error rate can be highly variable, depending on what observations are included.
2.  Since only a subset of observations are used to fit the model, validation set error rate may tend to overestimate the test error rate for the model fit on the entire dataset.

Cross validation addresses these two issues.

#### Leave one out Cross Validation (LOOCV)

We split the data into two parts. Instead of creating just two subsets of comparable size, we use a single observation $(x_1, y_1)$ for the validation set, and the remaining observations $\{(x_2,y_2),…(x_n, y_n)\}$ to make up the training set. We fit our learning method to the $n-1$ training observations and we make a prediction $\hat{y}_1$ to the excluded observation using its value of $x_1$.

Because we did not use the observation in the fitting process, the MSE is approximately unbiased for the test error. However, this method by itself is highly variable.

```{r}
## A very simple example
set.seed(123)
x = rnorm(100)
y = .5 + 2*x + rnorm(100)

dat = data.frame(y = y, x = x)
train_mse = vector(mode = "logical", length = nrow(dat))
test_mse = vector(mode = "logical", length = nrow(dat))
for(i in 1:nrow(dat)){
  out = dat[i,]
  # print(out)
  loo = dat[-i,]
  fit = lm(y~x, data = loo)
  train_mse[i] = mean((fit$model$y - predict(fit)))
  test_mse[i] = mean((out[["y"]]- predict(fit, newdata = out))^2)
}
round(mean(train_mse),3)
mean(test_mse)
```

LOOCV has the potential to be expensive to implement because we have to fit our model $n$ times. With least squares linear or polynomial regression, it turns out that a shortcut makes the cost of LOOCV the same as a single model fit.

$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^n\left(\frac{y_i - \hat{y}_i}{1-h_i} \right)^2
$$ Here $\hat{y}_i$ is the $ith$ fitted value from the original least squares fit, and $h_i$ is the leverage defined by

$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i' = 1}^n (x_{i'} - \bar{x})^2}
$$ The shortcut equation is almost the same as the usual MSE except that the $ith$ residual is divided by $1-h_i$. Leverage ($h_i$) is between $\frac{1}{n}$ and 1. It reflects the amount that an observation influences its own fit. High leverage points' residuals are inflated in the formula for the equality to hold.

### k-Fold Cross Validation

Since LOOCV is expensive, an alternative is k-fold CV. The approach involves randomly dividing our set of observations into $k$ folds of approximately equal size. The first fold is the validation set and the learning method is fit on to the remaining $k-1$ folds. MSE is then computed on the observations in the hold out fold. We repeat this procedure k times for each different group and then calculate our estimate by taking the average of the values

$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^kMSE_i
$$ Perhaps unsurprisingly, LOOCV is a k-fold cross validation where k is the same as n.

#### Bias-Variance Trade-off for k-fold Cross Validation

k-fold CV often does better than LOOCV on test error because of the bias variance trade-off. While LOOCV gives approximately unbiased estimates of the test error, because we are fitting on a single unit each time the procedure will likely have high variance. Conversely, while k-fold cross validation contains an intermediate amount of bias (each training set contains \$ \approx \frac{(k-1)n}{k}\$ observations) whenever $k < n$ the LOOCV procedure will have higher variance than k-fold CV.

Typically we set k-fold to 5 or 10 which as rules of thumbs tend to yield test error estimates that suffer from neither excessively high bias nor from high variance.

### Cross Validation for Classification

Same general ideas hold, except we use accuracy error.

## Chapter 6: Ridge Regression and Lasso

### Ridge Regression

Recall that the least squares fitting procedure finds appropriate weights $\beta_0,…,\beta_n$ by minimizing the residual sum of squares (RSS)

$$
RSS = \sum_{i=1}^n\left(y_i -\beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2
$$

Ridge regression uses a different loss function. Ridge regression finds coefficients $\hat{\beta}^R$ that minimizes:

$$
\sum_{i=1}^n\left(y_i -\beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda \sum_{j=1}^p\beta_j^2
$$ Here $\lambda$ is a tuning parameter. Staring at the equation we see that ridge regression trades off two different criteria. We want coefficient estimates that will fit the data well, but the second term is a shrinkage penalty. It is small whenever the $\beta_i$ are close to 0 and so has the effect of shrinking the estimates towards 0. When $\lambda=0$ there is no effect and it is the same estimator as linear regression. As $\lambda \rightarrow \infty$, the impact of the penalty grows so the ridge regression coefficient estimates will approach 0. The shrinkage penalty is not applied to the intercept.

We often center the predictors to have mean zero before ridge regression is performed. In this case, the estimated intercept is:

$$
\hat{\beta_0} = \bar{y}
$$ We do this because ridge regression coefficients are not scale equivalent. Ridge regression coefficients depend not only on the value of $\lambda$ but also on the scaling of the $jth$ predictor. It may also depend on the values of other predictors. We scale via the formula:

$$
\tilde{x}_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij} - \bar{x}_j)^2}}
$$ The denominator is the estimated standard deviation of the $jth$ predictor so all standardized predictors will have a standard deviation of 1. An implication is that the final fit will not depend on the scale of the predictors.

##### Application

TODO: Insert these in

```{r, cache = T}
Hitters = na.omit(ISLR2::Hitters)

##. standardize all predictors 
center = function(x){
  ## Function to center a vector around
  ## a mean of 0 
  return(x - mean(x))
}

standardize = function(x){
  centered = center(x)
  ## To match R scale() implementation
  ## divide by n-1 
  var = sum(center(x)^2)/(length(x)-1)
  sd = sqrt(var)
  return(center(x)/sd)
}

x = model.matrix(Salary ~ ., Hitters)[,-1]
y = Hitters[["Salary"]]
grid = 10^seq(10, -2, length = 100)
## Set alpha to 0 to get ridge. Set alpha to 1 to get lasso
ridge.mod = glmnet(x,y, alpha = 0, lambda = grid)
plot(ridge.mod)
lasso.mod = glmnet(x,y, alpha = 1, lambda = grid)
plot(lasso.mod)
```

Using cross-validation to find $\lambda$

```{r}
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]
set.seed(1)

### Ridge 
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
```

Now do the same with the Lasso

```{r}
### Lasso 
lasso.mod = glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred-y.test)^2)
```

#### Why does Ridge Regression improve over OLS

Ridge regression's advantage is in the bias-variance tradeoff. As $\lambda$ increases the flexibility of the ridge regression fit decreases.

-   The variance decreases and the bias of the estimate increases.

Because the test MSE is composed of both bias and variance, a substantial decrease in the variance will improve predictions on this measure of success.

In general, when the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. A small change in the training data can cause a large change in the coefficient estimates. Ridge regression works best in situations where the OLS estimate is high variance.

The primary disadvantage for ridge regression is that it includes all $p$ predictors in the final model, which can create a challenge for model interpretation when $p$ is large.

### The Lasso

Unlike ridge regression the lasso minimizes the following quantity:

$$
\sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij})^2 + \lambda \sum_{j=1}^p|\beta_j|
$$ The difference between ridge and lasso is the penalty term. In the lasso, we use the L1 norm instead of the L2 norm. In the lasso, the penalty has the effect of forcing some coefficient estimates to be exactly equal to zero when the tuning parameter is sufficiently large. A consequence is that the lasso is a variable selection model that yields sparse models. Depending on the value of $\lambda$ the lasso can produce a model involving any number of variables, while ridge regression will always include all predictors in the model even though the magnitude of the coefficient estimates will depend on $\lambda$.

### Comparing lasso and ridge regression

Both the lasso and ridge regression will often lead to qualitatively similar behavior in that as $\lambda \rightarrow \infty$ the variance decreases and the bias increases. In general neither ridge or lasso will universally dominate the other.

-   The lasso will often perform better when there are relatively small number of predictors that have substantial coefficients and the remaining predictors have coefficients close to 0. Lasso like ridge yields a decrease in variance at the expense of a increase in bias when least squares has high variance.

-   Ridge will perform better when the response is a function of many predictors with similar coefficient sizes.

Cross validation can be used to determine which approach is better. Generally speaking, ridge regression more or less shrinks every dimension by the same proportion whereas the lasso more or less shrinks all coefficients towards zero by a similar amount and sufficiently small coefficients go to 0.

### Selecting the tuning parameter

Use cross validation on the penalty term. Create a grid of $\lambda$ values and compute the cross validation error for each as described in the @sec-cv section.

## Chapter 8: Tree Based Methods

### CART

Building a regression tree proceeds loosely in two steps.

1.  Divide the predictor space into J distinct and non-overlapping regions $R_1, …, R_j$.
2.  For every observation that falls into the region $R_j$ make the same prediction which is simply the mean of the response values for the training observation in $R_j$.

To construct regions we divide the predictor space into boxes, the goal of which is to find boxes that minimize the RSS $\sum_{j=1}^J\sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$

It is computational infeasible to consider every possible partition of the feature space, so we use a top down greedy approach called recursive binary splitting. This begins at the top of the tree and at each step the best split is made at that particular step instead of looking ahead to a possible better split at a future node.

For recursive binary splitting, we first select the predictor and the cutpoint such that splitting the predictor space leads to the greatest possible reduction in RSS. We repeat this process for all of our regions until a stopping criterion is reached.

We prune trees via cost complexity pruning. Instead of considering every possible subtree, we consider the sequence of trees indexed by a non-negative tuning parameter $\alpha$. The general algorithm is laid out on page 333.

Classification trees are made via a similar process except that we use it to predict a qualitative rather than quantitative response. In addition, we cannot use RSS as our criterion for binary splits, so we use the binary entropy or Gini indexing instead.

### Bagging

Decision trees can suffer from high variance. Bootstrap aggregation (bagging) is a general purpose procedure fro reducing the variance of a statistical learning methods.We generate B different bootstrapped training data sets, train our method on the $b^{th}$ bootstrapped training set in order to get $\hat{f}^{*b}(x)$ and then average over all predictions to obtain $\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}B\hat{f}^{*b}(x)$

Bagging is particularly useful with decision trees. We construct B regression trees using B boostrapped training sets and average the resulting predictions. Each tree has high variance and low bias, and averaging over all trees reduces variances. For qualitative trees we take a majority vote for the average class prediction.

#### Out of Bag Error Estimation

On average, each bagged tree makes use of about 2/3 of the observations. The remaining 1/3 of the observations not used to fit a given bagged tree are referred to as out of bag (OOB) observations. We can use this to calculate the overall OOB MSE or classification error, which is a valid estimate of the test error for the bagged model because the response for each observation is predicted using only the trees not fit using that observation.

### Boosting

In boosting, we grow trees sequentially using information from previously grown trees. We do not bootstrap sample; instead each tree is fit on a modified version of the original dataset.

Unlike fitting a large decision tree, the boosting approach learns slowly. Given the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes determined by the max node size parameter n the algorithm. By fitting small trees to the residuals, we slowly improve our model fit in areas where it does not perform well.

At each iteration of boosting, the data are reweighted or modified based on how well the current models are predicting each point. Each new iteration prioritizes data points not yet accurately predicted/classified. Models are added together, and sometimes weighted based on individual accuracy.

#### Algorithm

1.  Set $\hat{f}(x) = 0, r_i = y_i$ for all $i$ in the training set.

2.  For b = 1,2,..., B repeat:

-   Fit a tree with d splits (d + 1 terminal nodes) to the training data (X,r).
-   Update model by adding in a shrunken version of the new tree $\hat{f}(x) = \hat{f}(x) + \lambda \hat{f}^b(x)$
-   Update the residuals $r_i = r_i + \lambda \hat{f}^b(x)$

3.  Output the boosted model. $\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)$

We have three parameters.

1.  The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B.

2.  The shrinkage parameter $\lambda$, a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 and 0.001 and the right choice can depend on the problem. Very small $\lambda$ using a very large value of B in order to achieve good performance.

3.  The interaction depth/number $d$ of splits in each tree, which controls the complexity of the boosted ensemble. Often $d = 1$ works well, in which case each tree is a stump, consisting of a single split. It controls the interaction order of the boosted model because d splits can involve at most d variables.

In general, the rough recommendation for interaction depth is between 3 and 7. The model should be trained with too many iterations initially, and cross validation should be used to find the final number of boosting iterations. The learning rate has an important impact on optimal B. Since it is possible for gradient boosted trees to overfit, use some preliminary exploration to determine suitable values for interaction depth, data sub-sampling, and the learning rate. Leave B as the primary tuning parameter.

### Random Forests

Random forests provide an improvement over bagged trees by decorrelating the trees. We build a number of decision trees on bootstrapped random samples. When building the tree, each time a split is considered we take a random sample of m predictors to be split candidates from the full set of p predictors. We typically choose $m \approx \sqrt{p}$. The main difference between Random Forests and bagging is the size of the predictor subset. When $m = p$ random forests are equivalent to bagging.

There are multiple sources of randomness in a Random Forest. The first is that since random forests are an application of bagging, for every tree data are randomly re-sampled. This is one of the ways that the trees are decorrelated from each other. The second is that the subset of predictors used is randomly sampled for consideration at each split.

#### Algorithm

Let the training data consist of $p$ inputs and an outcome, for each of N observations $(\textbf{x}_i, y_i)$ for $i = 1,2,...N$.

1.  For b = 1 to B:

-   Draw bootstrap resample of size N from the training data.
-   Fit a tree $T_b$ to the bootstrapped data by recursively repeating the following steps for each node until the minimum node size is reached.
    -   Select m variables at random from the p predictors.
    -   Pick the best variable/split point among m.
    -   Split the node into two new nodes.

2.  Output the ensemble/committee of trees $\{T_b\}^B_1$.
3.  Make a prediction at a new point $\textbf{x}_0$

Random Forests tend to perform poorly in situations where m is set to be small and where the number of possible variables is large, but the fraction of relevant variables is small.

### BART

Bayesian Additive Regression Trees (BART) are another ensemble method. In BART we have $K$ number of regression trees and $B$ number of iterations for which the algorithm runs. $\hat{f}^B_k(x)$ represents the prediction at $x$ for the $k^{th}$ regression tree at the $b^{th}$ iteration. At the end of each iteration we sum all K trees.

We typically throw away the first few BART prediction models as they are part of the burn-in period.

#### Algorithm

1.  Let $\hat{f}^1_1(x) = ... = \hat{f}^1_K(x) = \frac{1}{nK}\sum_{i=1}^ny_i$

2.  Compute $\hat{f}^1(x) = \sum_{k=1}^K \hat{f}^1_k(x) = \frac{1}{n}\sum_{i=1}^n y_i$

3.  For b = 2, ... B:

-   For k = 1,2,...,K:
    -   For i = 1,..., n, compute the current partial residual $r_i = y_i - \sum_{k' < k}\hat{f}^b_{k'}(x_i) - \sum_{k' > k}\hat{f}^{b-1}_{k'}(x_i)$

    -   Fit a new tree to $r_i$ by randomly perturbing the $k^{th}$ tree from the previous iteration. Perturbations that improve the fit are favored.
-   Compute $\hat{f}^b(x) = \sum_{k=1}^k \hat{f}^b_k(x)$

4.  Compute the mean after L burn-in samples. $\hat{f}(x) = \frac{1}{B-L}\sum_{b=L+1}^B\hat{f}%b(x)$

## Chapter 12: Unsupervised Learning

Most of the book concerns supervised learning methods. In these methods there is a set of $p$ features measured on $n$ on observations and a known response $Y$ also measured on those observations. In unsupervised learning, we only have a set of features without a ground truth response value.

Unsupervised learning is more subjective and there is no simple goal for the analysis. UL is often performed as part of initial exploratory data analysis.

### Principal Components Analysis

When faced with a large set of correlated variables, principal components allows us to summarize the data with a smaller number of representative variables in the original set. PCA refers to the process by which principal components are computed and the subsequent use of these components in understanding the data.

Suppose we wish to visualize $n$ observations with measurements on a set of $p$ features. There are ${p \choose 2} = p(p-1)/2$ ways to do this. We would like to find a low-dimensional representation of the data that captures as much of the information as possible. Each of the dimensions found by. PCA is a linear combination of the $p$ features.

The first principal components of the set of features is the normalized linear combination of the features $$Z_1 = \phi_{11}X_1 + …+ \phi_{p1}X_p$$

that has the largest variance. The $\phi$ terms are the loading of the first principal component. We constrain them so that the sum of squares equals 1. If we do not do this, setting these elements to a large value could lead to a large variance.

We assume that each variable has been centered to have mean 0 and look for features of the same that has the largest sample variance subject to the normalizing constraint. This is solved via an eigen decomposition.[^8]

[^8]: The actual details is outside the scope of this question.

After the first principal component has been determined, we can find the second principal component which is the linear combination that has maximal variance out of all linear combinations that are uncorrelated with the first principal component.

An alternative interpretation of principal components is that they provide low dimensional linear surfaces that are closest to the observations. The first principal component loading vector is the line in p-dimensional space that is closest to the n observations. The appeal of this interpretation is that the dimension of data that it answers the question of what single dimension of data lies as close as possible to all data points. The first two dimensions span the plane that is closest to the n observations in terms of average squared euclidean distance. We can write this description for M principal components to get the best M-dimensional approximation in terms of Euclidean distance to the $i^{th}$ observation $x_{ij}$. The representation can be written as $x_{ij} \approx \sum_{m=1}^M z_{im}\phi_{jm}$

#### Proportion of Variance Explained

We can ask a general question. How much information in a given data set is lost by projecting the observations onto the first few principal components? In other words, how much variance in the data is not contained in the first few principal components. The total variance present in a dataset (with mean zero centered variables) is $\sum_{j=1}^p V(X_j) = \sum_{j=1}^p\frac{1}{n}\sum_{i=1}^n x^2_{ij}$ 

The variance explained by the $m^th$ principal component is: $\frac{1}{n}\sum_{i=1}^n z^2_{im} = \frac{1}{n}\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2$

The Proportion of Variance Explained (PVE) of the $m^th$ principal component is given by

$$\begin{aligned}

\frac{\sum_{i=1}^nz^2_{im}}{\sum_{j=1}^p\sum_{i=1}^n x^2_{ij}} = \frac{\sum_{i=1}^n\left(\sum_{j=1}^p \phi_{jm}x_{ij} \right)^2}{\sum_{j=1}^p\sum_{i=1}^nx^2_{ij}}
\end{aligned}$$

The PVE of each principal component is non-negative. In total there are $min(n-1, p)$ principal components and their PVEs sum to 1. 

We can decompose the variance of data into the variance of the first M principal components plus the MSE of the M-dimensional approximation. 

$$\begin{aligned}
\sum_{j=1}^p \frac{1}{n}\sum_{i=1}^nx^2_{ij} = \sum_{m=1}^M\frac{1}{n}\sum_{i=1}^nz^2_{im} + \frac{1}{n}\sum_{j=1}^p\sum_{i=1}^n\left(x_{ij} - \sum_{m=1}^Mz_{im}\phi_{jm} \right)^2
\end{aligned}$$

The first term is the variance of data. The second term is the variance of the first M PCs and the third term is the MSE of M-dimensional approximation. The first term is fixed, so by maximizing the variance of the first M principal components, we mechanically minimize the MSE of the M dimensional approximation and vice versa. 

It is also the case the PVE defined above is equivalent to $1 - \frac{RSS}{TSS}$ where TSS represents the total sum of squared elements of $\textbf{X}$ and RSS represents the residual sum of squares of the M-dimensional approximation given by principal components. This means we can interpret the PVE as the $R^2$ of the approximation for $\textbf{X}$ given by the first M principal components. 

#### Scaling Variables

Whenever we perform PCA, we should center the variables to have mean 0. The results obtained when we perform PCA will also depend on whether the variables have been individually scaled. This is in contrast to some other supervised and unsupervised algorithms in which scaling has no effect. Because this is undesirable behavior, we typically scale each variable to have standard deviation one before we perform PCA, unless every variable is measured already in the same units. 

#### Deciding how many Principal Components to Use

There is no single answer to how many principal components to use in a problem. We typically decide on the number of principal components by examining a *scree plot*. We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation of the data. The drop in the scree plot is often referred to as the "elbow." 
### Clustering Methods

Clustering refers to a broad set of techniques for finding subgroups, called clusters, in a data set. When we cluster observations of a data set, we partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. 

Clustering looks to find homogeneous subgroups among the observations. This differs from PCA whose mechanism is to find a low-dimensional representation of the observations that explain a good fraction of the variance. 

#### K-Means Clustering

In K-means the idea is that a good clustering is one for which the within-cluster variation is as small as possible. The within cluster variation for cluster $C_k$ is a measure $W(C_k)$ of the amount by which the observations within a cluster differ from each other. We seek to minimize this problem by partitioning the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible. 

We define within cluster variation as the squared Euclidean distance between observations. $W(C_k) = \frac{1}{|C_k|}\sum_{i, i' \in C_k}\sum_{j=1}^p(x_{ij} - x_{i'j})^2$ where $|C_k|$ denotes the number of observations in the $k^th$ cluster. 

The overall optimization problem of K-means is thus 

$$\begin{aligned}
\arg\min_{C_1, .., C_k} \left\{\sum_{k=1}^K \frac{1}{|C_k|}\sum_{i,i' \in C_k}\sum_{j=1}^p(x_{ij} - x_{i'j})^2 \right\}
\end{aligned}$$

The algorithm to solve this optimization problem is: 

##### Algorithm 

1. Randomly assign a number from 1 to K, to each observation. These are initial cluster assignments for the observations. 

2. Iterate until the cluster assignments stop changing:
  - For each of the K clusters, compute the cluster centroid. The $k^th$ cluster centroid is the vector of p feature means for the observation in the $k^th$ cluster. 
  - Assign each observation to the cluster whose centroid is closest (where closest is defined by Euclidean distance)
  
The K-means algorithm finds a local rather than a global optimum. The results thus depend on the initial random cluster assignments of each observations. As a result, we want to run the algorithm multiple times from different random initial configurations. Then we select the solution for which the objective function is smallest. 

#### Hierarchical Clustering

Hierarchical clustering is an alternative approach that does not require we commit to a specific choice of K. The most common type of hierarchical clustering is agglomerative (bottom-up) clustering.

##### Algorithm 

Define some sort of dissimilarity measure between each pair of observations (usually Euclidean distance). 

1. Starting at the bottom of the dendrogram, each of the n observations is treated as its own cluster. The two clusters that are most similar to each other are fused so that there are now n-1 clusters. 
2. Repeat until all observations are fused into a single cluster. 

Dissimilarity is defined by one of four (usually) linkage measures--complete, average, single, and centroid. Centroid is usually not used by statisticians. 

Complete: Maximial intercluster dissimilarity. Compute all pairwise similiarities between the observations in cluster A and the observation in cluster B. Record the largest of these similiarity. 

Single: Minimal intercluster dissimiliarity. Compute all pairwise similiarities between the observations in cluster A and the observations in cluster B, record the smallest of these dissimiliarities. Single linkage can result in extended, trailing clusters in which single observations are fused one at a time. 

Average: Mean intercluster dissimiliarity. Compute all pairwise dissimiliarities between the observations in cluster A and the observations in cluster B. Record the average of these dissimiliarities. 

Centroid: Dissimiliarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Can result in undesirable inversions whereby two clusters are fused as a height below either of the individual clusters in the dendrogram. 

#### Choice of Dissimilarity Measure

Normally, we use Euclidean distance, but we could use other dissimilarity measures. Correlation based distance considers two observations to be similar if their features are highly correlated even though the observed values may be far apart in terms of Euclidean distance. 

The choice of dissimilarity matters because it has a strong effect on the resulting dendrogram. 

#### Practical Issues in Clustering

In order to perform clustering, the analyst must make some decisions. 

- Should the observations or features be standardized in some way? 

- In hierarchical clustering, what dissimilarity measure should be used? What type of linkage should be used? Where should we cut the dendrogram to obtain clusters? 

- In K-means clustering, how many clusters should we look for in the data? 

Each of these decisions has a strong impact on the results obtained. In practice, try several different choices and look for the one with the most useful or interpretable solutions. 
