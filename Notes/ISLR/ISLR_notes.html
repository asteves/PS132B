<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ISLR Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="ISLR_notes_files/libs/clipboard/clipboard.min.js"></script>
<script src="ISLR_notes_files/libs/quarto-html/quarto.js"></script>
<script src="ISLR_notes_files/libs/quarto-html/popper.min.js"></script>
<script src="ISLR_notes_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="ISLR_notes_files/libs/quarto-html/anchor.min.js"></script>
<link href="ISLR_notes_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="ISLR_notes_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="ISLR_notes_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="ISLR_notes_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="ISLR_notes_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Chapters</h2>
   
  <ul>
  <li><a href="#libraries" id="toc-libraries" class="nav-link active" data-scroll-target="#libraries"><span class="toc-section-number">1</span>  Libraries</a></li>
  <li><a href="#chapter-2-statistical-learning" id="toc-chapter-2-statistical-learning" class="nav-link" data-scroll-target="#chapter-2-statistical-learning"><span class="toc-section-number">2</span>  Chapter 2: Statistical Learning</a>
  <ul class="collapse">
  <li><a href="#figure-2.1" id="toc-figure-2.1" class="nav-link" data-scroll-target="#figure-2.1"><span class="toc-section-number">2.1</span>  Figure 2.1</a></li>
  <li><a href="#figure-2.2" id="toc-figure-2.2" class="nav-link" data-scroll-target="#figure-2.2"><span class="toc-section-number">2.2</span>  Figure 2.2</a></li>
  <li><a href="#why-estimate-f" id="toc-why-estimate-f" class="nav-link" data-scroll-target="#why-estimate-f"><span class="toc-section-number">2.3</span>  Why Estimate <span class="math inline">f</span>?</a></li>
  <li><a href="#how-do-we-estimate-f" id="toc-how-do-we-estimate-f" class="nav-link" data-scroll-target="#how-do-we-estimate-f"><span class="toc-section-number">2.4</span>  How do we estimate <span class="math inline">f</span>?</a></li>
  <li><a href="#supervised-and-unsupervised-learning" id="toc-supervised-and-unsupervised-learning" class="nav-link" data-scroll-target="#supervised-and-unsupervised-learning"><span class="toc-section-number">2.5</span>  Supervised and Unsupervised Learning</a></li>
  <li><a href="#regression-vs.-classification" id="toc-regression-vs.-classification" class="nav-link" data-scroll-target="#regression-vs.-classification"><span class="toc-section-number">2.6</span>  Regression vs.&nbsp;Classification</a></li>
  <li><a href="#assessing-model-accuracy" id="toc-assessing-model-accuracy" class="nav-link" data-scroll-target="#assessing-model-accuracy"><span class="toc-section-number">2.7</span>  Assessing Model Accuracy</a></li>
  <li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff" class="nav-link" data-scroll-target="#bias-variance-tradeoff"><span class="toc-section-number">2.8</span>  Bias-Variance Tradeoff</a></li>
  <li><a href="#lab" id="toc-lab" class="nav-link" data-scroll-target="#lab"><span class="toc-section-number">2.9</span>  Lab</a></li>
  <li><a href="#indexing-data" id="toc-indexing-data" class="nav-link" data-scroll-target="#indexing-data"><span class="toc-section-number">2.10</span>  Indexing Data</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">2.11</span>  Exercises</a></li>
  </ul></li>
  <li><a href="#chapter-3-linear-regression" id="toc-chapter-3-linear-regression" class="nav-link" data-scroll-target="#chapter-3-linear-regression"><span class="toc-section-number">3</span>  Chapter 3: Linear Regression</a>
  <ul class="collapse">
  <li><a href="#the-math" id="toc-the-math" class="nav-link" data-scroll-target="#the-math"><span class="toc-section-number">3.1</span>  The Math</a></li>
  <li><a href="#multiple-regression" id="toc-multiple-regression" class="nav-link" data-scroll-target="#multiple-regression"><span class="toc-section-number">3.2</span>  Multiple Regression</a></li>
  <li><a href="#a-word-on-interactions" id="toc-a-word-on-interactions" class="nav-link" data-scroll-target="#a-word-on-interactions"><span class="toc-section-number">3.3</span>  A word on interactions</a></li>
  </ul></li>
  <li><a href="#chapter-4-classification" id="toc-chapter-4-classification" class="nav-link" data-scroll-target="#chapter-4-classification"><span class="toc-section-number">4</span>  Chapter 4: Classification</a>
  <ul class="collapse">
  <li><a href="#why-not-linear-regression" id="toc-why-not-linear-regression" class="nav-link" data-scroll-target="#why-not-linear-regression"><span class="toc-section-number">4.1</span>  Why Not Linear Regression?</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="toc-section-number">4.2</span>  Logistic Regression</a></li>
  <li><a href="#estimating-a-logistic-regression" id="toc-estimating-a-logistic-regression" class="nav-link" data-scroll-target="#estimating-a-logistic-regression"><span class="toc-section-number">4.3</span>  Estimating a logistic regression</a></li>
  <li><a href="#multiple-logistic-regression" id="toc-multiple-logistic-regression" class="nav-link" data-scroll-target="#multiple-logistic-regression"><span class="toc-section-number">4.4</span>  Multiple Logistic Regression</a></li>
  </ul></li>
  <li><a href="#chapter-5-resampling-methods" id="toc-chapter-5-resampling-methods" class="nav-link" data-scroll-target="#chapter-5-resampling-methods"><span class="toc-section-number">5</span>  Chapter 5: Resampling Methods</a>
  <ul class="collapse">
  <li><a href="#sec-cv" id="toc-sec-cv" class="nav-link" data-scroll-target="#sec-cv"><span class="toc-section-number">5.1</span>  Cross Validation</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation"><span class="toc-section-number">5.2</span>  k-Fold Cross Validation</a></li>
  <li><a href="#cross-validation-for-classification" id="toc-cross-validation-for-classification" class="nav-link" data-scroll-target="#cross-validation-for-classification"><span class="toc-section-number">5.3</span>  Cross Validation for Classification</a></li>
  </ul></li>
  <li><a href="#chapter-6-ridge-regression-and-lasso" id="toc-chapter-6-ridge-regression-and-lasso" class="nav-link" data-scroll-target="#chapter-6-ridge-regression-and-lasso"><span class="toc-section-number">6</span>  Chapter 6: Ridge Regression and Lasso</a>
  <ul class="collapse">
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression"><span class="toc-section-number">6.1</span>  Ridge Regression</a></li>
  <li><a href="#the-lasso" id="toc-the-lasso" class="nav-link" data-scroll-target="#the-lasso"><span class="toc-section-number">6.2</span>  The Lasso</a></li>
  <li><a href="#comparing-lasso-and-ridge-regression" id="toc-comparing-lasso-and-ridge-regression" class="nav-link" data-scroll-target="#comparing-lasso-and-ridge-regression"><span class="toc-section-number">6.3</span>  Comparing lasso and ridge regression</a></li>
  <li><a href="#selecting-the-tuning-parameter" id="toc-selecting-the-tuning-parameter" class="nav-link" data-scroll-target="#selecting-the-tuning-parameter"><span class="toc-section-number">6.4</span>  Selecting the tuning parameter</a></li>
  </ul></li>
  <li><a href="#chapter-8-tree-based-methods" id="toc-chapter-8-tree-based-methods" class="nav-link" data-scroll-target="#chapter-8-tree-based-methods"><span class="toc-section-number">7</span>  Chapter 8: Tree Based Methods</a>
  <ul class="collapse">
  <li><a href="#cart" id="toc-cart" class="nav-link" data-scroll-target="#cart"><span class="toc-section-number">7.1</span>  CART</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="toc-section-number">7.2</span>  Bagging</a></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="toc-section-number">7.3</span>  Boosting</a></li>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="toc-section-number">7.4</span>  Random Forests</a></li>
  <li><a href="#bart" id="toc-bart" class="nav-link" data-scroll-target="#bart"><span class="toc-section-number">7.5</span>  BART</a></li>
  </ul></li>
  <li><a href="#chapter-12-unsupervised-learning" id="toc-chapter-12-unsupervised-learning" class="nav-link" data-scroll-target="#chapter-12-unsupervised-learning"><span class="toc-section-number">8</span>  Chapter 12: Unsupervised Learning</a>
  <ul class="collapse">
  <li><a href="#principal-components-analysis" id="toc-principal-components-analysis" class="nav-link" data-scroll-target="#principal-components-analysis"><span class="toc-section-number">8.1</span>  Principal Components Analysis</a></li>
  <li><a href="#clustering-methods" id="toc-clustering-methods" class="nav-link" data-scroll-target="#clustering-methods"><span class="toc-section-number">8.2</span>  Clustering Methods</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ISLR Notes</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="libraries" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="libraries"><span class="header-section-number">1</span> Libraries</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="chapter-2-statistical-learning" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="chapter-2-statistical-learning"><span class="header-section-number">2</span> Chapter 2: Statistical Learning</h2>
<section id="figure-2.1" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="figure-2.1"><span class="header-section-number">2.1</span> Figure 2.1</h3>
<p>While reading the main textbook, it will be helpful to make a list of flashcards of basic terms and definitions that are <em>italicized</em> in the chapters. The first key set of terms are input variables and output variables. <em>Input variables</em> go by lots of names, but the main commonality is that input variables go on the right hand side of an equation.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <em>Output variables</em> are the outputs of the function we seek to learn using the input variables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Anything that follows a # inside a R code block is a comment </span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="do">## We use them to provide notes to ourselves or others </span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="do">## about what a line or lines of code do to help solve </span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="do">## the problem we are working on. </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="do">## I personally find ## easier to read than #. </span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Most of my comments here relate to where you find certain functions</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="do">## in the set of tidyverse packages. </span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Learning what you want to do is as important </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="do">## as learning where functions are located. </span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="do">## readr::read_csv()</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>ads <span class="ot">=</span> <span class="fu">read_csv</span>(<span class="st">"https://www.statlearning.com/s/Advertising.csv"</span>,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">show_col_types =</span> <span class="cn">FALSE</span>) <span class="sc">|&gt;</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Remove the row names </span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="do">## dplyr::select()</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="st">`</span><span class="at">...1</span><span class="st">`</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>ads <span class="sc">|&gt;</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  <span class="do">## tidyr::pivot_longer()</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>sales, </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">"type"</span>, </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">"spend"</span>) <span class="sc">|&gt;</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  <span class="do">## dplyr::mutate, dplyr::case_when(), stringr::str_detect, stringr::str_to_title()</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">type =</span> <span class="fu">case_when</span>(</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">str_detect</span>(type, <span class="st">"TV"</span>)<span class="sc">~</span><span class="st">"TV"</span>,</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cn">TRUE</span><span class="sc">~</span><span class="fu">str_to_title</span>(type)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">|&gt;</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>  <span class="do">## ggplot2 for all functions unless noted</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(spend, </span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>             sales, </span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>             <span class="at">group =</span> type))<span class="sc">+</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>  <span class="do">## forcats::fct_rev()</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span><span class="fu">fct_rev</span>(type), </span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>             <span class="at">scales =</span> <span class="st">"free"</span>, </span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>             <span class="at">strip.position =</span> <span class="st">"bottom"</span>)<span class="sc">+</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()<span class="sc">+</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, </span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>              <span class="at">se =</span> F)<span class="sc">+</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">strip.background =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="at">strip.placement =</span> <span class="st">"outside"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The base version of this figure is slightly more annoying, but that’s just my opinion. You may prefer it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ads <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"https://www.statlearning.com/s/Advertising.csv"</span>,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>) </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>ads <span class="ot">=</span> ads[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>ads_long <span class="ot">=</span> ads <span class="sc">|&gt;</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">reshape</span>(<span class="at">direction =</span> <span class="st">"long"</span>, </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">v.names =</span> <span class="st">"spend"</span>, </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">timevar =</span><span class="st">"type"</span>, </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">times =</span> <span class="fu">names</span>(ads[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)]),</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>          <span class="at">varying =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>ads_long<span class="sc">$</span>type <span class="ot">=</span> <span class="fu">ifelse</span>(ads_long<span class="sc">$</span>type <span class="sc">!=</span> <span class="st">"TV"</span>, </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                       tools<span class="sc">::</span><span class="fu">toTitleCase</span>(ads_long<span class="sc">$</span>type), </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                       ads_long<span class="sc">$</span>type)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>ads_long <span class="sc">|&gt;</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="do">## ggplot2 for all functions unless noted</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(spend, </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>             sales, </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>             <span class="at">group =</span> type))<span class="sc">+</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>  <span class="do">## forcats::fct_rev()</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span><span class="fu">fct_rev</span>(type), </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>             <span class="at">scales =</span> <span class="st">"free"</span>, </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>             <span class="at">strip.position =</span> <span class="st">"bottom"</span>)<span class="sc">+</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()<span class="sc">+</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>              <span class="at">se =</span> F)<span class="sc">+</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">strip.background =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="at">strip.placement =</span> <span class="st">"outside"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="figure-2.2" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="figure-2.2"><span class="header-section-number">2.2</span> Figure 2.2</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>income <span class="ot">=</span> <span class="fu">read_csv</span>(<span class="st">"https://www.statlearning.com/s/Income1.csv"</span>, </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">show_col_types =</span> <span class="cn">FALSE</span>) <span class="sc">|&gt;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Another way to remove the first column </span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="do">## tidyselect::contains()</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="fu">contains</span>(<span class="st">"."</span>))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>income <span class="sc">|&gt;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(Education, Income))<span class="sc">+</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Income"</span>)<span class="sc">+</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Years of Education"</span>)<span class="sc">+</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can think of the fundamental learning problem of interest that of learning some function that relates our output and input variables.</p>
<p><span class="math display">
\begin{aligned}
Y = f(X) + \epsilon
\end{aligned}
</span></p>
<p>Y corresponds to the output variable and <span class="math inline">X</span> is a vector of input variables. The <span class="math inline">\epsilon</span> (epsilon) is the random error term, for which we may or may not make an assumption about. Usually when we make assumptions it is that it is independent of <span class="math inline">X</span> and (trivially because we can always normalize a set of values) has mean zero.</p>
</section>
<section id="why-estimate-f" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="why-estimate-f"><span class="header-section-number">2.3</span> Why Estimate <span class="math inline">f</span>?</h3>
<section id="prediction" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="prediction"><span class="header-section-number">2.3.1</span> Prediction</h4>
<p>To be frank (and opinionated), prediction is the reason why we most of the time people are interested in machine learning. We have a set of input variables, and we use them to estimate our function of interest. Over the course of the semester, we will learn some different ways to estimate this function but the most common estimation strategy is some form of regression.</p>
<p>We determine if our prediction is any good by some metric. A common one might be accuracy. How often does our prediction get sufficiently close to our observed output variables. We break this up into <em>reducible</em> and <em>irreducible</em> errors. Reducible error is something we can make smaller, often by collecting more or better data. Irreducible error is something we are stuck with because it is a function of a variable we can never collect data about (that pesky <span class="math inline">\epsilon</span>).</p>
<p>We can decompose the expected error into reducible and irreducible parts.</p>
<p><span class="math display">
\begin{aligned}
E(Y-\hat{Y})^2 &amp;= E[f(X) + \epsilon - \hat{f}(X)]^2 \\
E(Y-\hat{Y})^2 &amp;= E[f(X) + \epsilon - \hat{f}(X)]E[f(X) + \epsilon - \hat{f}(X)] \\
E(Y-\hat{Y})^2 &amp;= E[f(X)^2 + 2f(X)\epsilon - 2f(X)\hat{f}(X) + \epsilon^2 - 2\epsilon\hat{f}(X) + \hat{f}(X)^2] \\
E(Y-\hat{Y})^2 &amp;= E[f(X)^2 - 2f(X)\hat{f}(X) + \hat{f}(X)^2] + E[\epsilon^2] \\
E(Y-\hat{Y})^2 &amp;= E[(f(X)^2 - \hat{f}(X))^2] + V[\epsilon] \\
E(Y-\hat{Y})^2 &amp;= [f(X)^2 - \hat{f}(X)]^2 + V[\epsilon]
\end{aligned}
</span></p>
<p>How did we do that?</p>
<ul>
<li><p>The second line expands out the squared term.</p></li>
<li><p>The third line applies the identity that <span class="math inline">(x+y-z)^2 = x^2 + 2xy - 2xz + y^2 -2yz + z^2</span>.</p></li>
<li><p>The fourth line does a couple of steps at once based on properties of the expectation operator: <span class="math inline">E[c] = c, E[x + y] = E[x] + E[y]</span> and then groups terms.</p></li>
<li><p>The fifth line substitutes in the appropriate definition of the variance and condenses the square. The sixth line applies the expectation of a constant property again.</p></li>
</ul>
<p>Irreducible error matters because the irreducible error bounds the accuracy of our predictions of Y, and we never actually know what it is in practice.</p>
</section>
<section id="inference" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="inference"><span class="header-section-number">2.3.2</span> Inference</h4>
<p>The book notes that another reason we care about the relationship between input and output variables is to learn something about their association. Note here the word “association” which does mean related but does not imply that the input variables cause the output variables. That is a separate problem of causal inference. People are sloppy all the time about this, but for the purpose of learning you should always be clear about when the inference you are drawing is causal.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</section>
</section>
<section id="how-do-we-estimate-f" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="how-do-we-estimate-f"><span class="header-section-number">2.4</span> How do we estimate <span class="math inline">f</span>?</h3>
<p>Suppose we observe a set of <em>n</em> different data points. We call these observations <em>training data</em> because we use them to train our estimation method how to estimate <em>f</em> our function of interest. In the book’s notation:</p>
<p><span class="math display">
\begin{aligned}
\{(x_1, y_1),...,(x_n, y_n)\}, x_i = (x_{i1}, ...x_{ip})^T
\end{aligned}
</span></p>
<p>That compact expression looks confusing if we have not seen them before, but all it is seeing is that we have a set of realizations of input variables <span class="math inline">X</span> and output variable <span class="math inline">y</span> and each <span class="math inline">X</span> is the realization of of the vector of input variables. Here’s an example in code.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## number of training data pairs</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="do">## tibble::tibble()</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>training_data <span class="ot">=</span> <span class="fu">tibble</span>(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">x3 =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3 <span class="sc">+</span> <span class="fu">runif</span>(n)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>) <span class="sc">|&gt;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="do">## dplyr::relocate()</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">relocate</span>(y, <span class="at">.before =</span> x1)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>training_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 4
       y      x1     x2     x3
   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
1  3.34  -0.560   1.72   1.22 
2  1.49  -0.230   0.461  0.360
3  1.39   1.56   -1.27   0.401
4  0.290  0.0705 -0.687  0.111
5 -0.848  0.129  -0.446 -0.556</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Base version </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">5</span> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>training_data <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">x3 =</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>training_data[[<span class="st">"y"</span>]] <span class="ot">=</span> training_data<span class="sc">$</span>x1 <span class="sc">+</span> training_data<span class="sc">$</span>x2 <span class="sc">+</span> training_data<span class="sc">$</span>x3 <span class="sc">+</span> <span class="fu">runif</span>(n)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>training_data <span class="ot">=</span> training_data[<span class="fu">union</span>(<span class="st">"y"</span>, <span class="fu">names</span>(training_data))]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>training_data </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           y          x1         x2         x3
1  3.3416954 -0.56047565  1.7150650  1.2240818
2  1.4928516 -0.23017749  0.4609162  0.3598138
3  1.3851238  1.55870831 -1.2650612  0.4007715
4  0.2898057  0.07050839 -0.6868529  0.1106827
5 -0.8476017  0.12928774 -0.4456620 -0.5558411</code></pre>
</div>
</div>
<p>The other part of that equation that might be strange to you is the <span class="math inline">T</span> at the end. This is the symbol for transpose, and is done because by convention we think of all of the operations as column operations.</p>
<section id="parametric-methods" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="parametric-methods"><span class="header-section-number">2.4.1</span> Parametric Methods</h4>
<p>Parametric means that we make some assumptions about the parameters of the function. For example, we might assume that the function is linear in parameters, or that no value can ever be above 42, or a host of other possible assumptions.</p>
<p>In parametric modeling, we first make an assumption about the functional form of the <span class="math inline">f</span> and then follow some procedure to estimate a model of that functional form.</p>
<p>What’s the benefit? Well since we have assumed that the problem must look a certain way, estimating the function is now much easier. The downside (and there is always a downside in everything) is that if we are wrong about this assumption we will be very wrong. Some parametric assumptions (linear function) tend to be used a lot.</p>
</section>
<section id="non-parametric-methods" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="non-parametric-methods"><span class="header-section-number">2.4.2</span> Non-parametric Methods</h4>
<p>Non-parametric methods do not make explicit assumptions about the functional form of the function of interest. Instead, they simply seek to estimate it in a way that flexibly fits the data without fitting <em>too well</em>. We will see what that means as we move through the book, but the word to get familiar with is <em>overfitting</em>.</p>
<p>Note that for both parametric and non-parametric methods the reason why we care about overfitting is that we are seeking to learn about some function <span class="math inline">f</span> based on our training data, not what the best functional fit of the data we have is. Overfitting only matters because we do not care about the training data for its own sake, but rather what it can tell us about future observations.</p>
</section>
</section>
<section id="supervised-and-unsupervised-learning" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="supervised-and-unsupervised-learning"><span class="header-section-number">2.5</span> Supervised and Unsupervised Learning</h3>
<p>Supervised learning implies that we have some observations for which we know the outputs. It is “supervised” because we are telling our machine assistant what is the correct answer. The vast majority of prediction problems are supervised learning problems. Unsupervised learning occurs when we do not have a correct output variable to predict, but still seek to understand relationships between variables or between observations. Unsupervised topic models would be an example of the latter in text analysis.</p>
</section>
<section id="regression-vs.-classification" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="regression-vs.-classification"><span class="header-section-number">2.6</span> Regression vs.&nbsp;Classification</h3>
<p>Because methods come from different disciplines, there is some confusing vocabulary distinctions. For the purpose of this book, when you hear a regression problem, you should think of trying to predict a continuous or quantitative output variable. Is more advertising associated with increased sales is a regression problem. In contrast a classification problem deals with ordinal or nominal data where the goal is the predict what bucket a set of observations might fall into. Will a person with a set of attributes be likely to get into Cal is a classification problem.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
</section>
<section id="assessing-model-accuracy" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="assessing-model-accuracy"><span class="header-section-number">2.7</span> Assessing Model Accuracy</h3>
<p>There are an infinite number of possible models that we can use to estimate our function of interest given our input variables and output variables. Therefore we need some way to determine which model is “best” and so also need some metric to define “best.” A common one in the regression setting is Mean Squared Error (MSE).</p>
<p><span class="math display">
\begin{aligned}
MSE = \frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(x_i))^2
\end{aligned}
</span></p>
<p>Mean Squared Error is an example of a “loss function.” By its name, you might infer correctly that the goal is to pick the model that has the smallest loss function of interest. Examine the equation for MSE to convince yourself that the MSE will be as low as possible if the prediction that our estimate gives is identical to the true value of our observed output variable.</p>
<p>As stated earlier, we do not care about the MSE of the training data per se. We care about the MSE of our learning method on unseen data, which is often referred to as <em>test</em> data. Supposing that we have a large number of training observation to estimate a model, we simply sub in new data pairs into the above equation to get the test MSE.</p>
<p>One key point regarding flexibility of models is that as the flexibility of our model increases (the fewer assumptions we put on functional form) we will observe a decrease in the training MSE and a U shape in the test MSE. <em>Overfitting</em> occurs when we have a low training MSE but a high test MSE.</p>
</section>
<section id="bias-variance-tradeoff" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="bias-variance-tradeoff"><span class="header-section-number">2.8</span> Bias-Variance Tradeoff</h3>
<p>The most important trade-off in all of machine learning is the Bias-Variance tradeoff. Bias refers to (loosely) how off our estimate of the true function would be as the number of training examples gets very large. Formally the bias of some statistic <span class="math inline">\theta</span> is $B[\theta] = E[\theta - \hat{\theta}]$. We can think of bias as the error introduced by approximating the problem of interest. Variance on the other hand refers to how much our estimate <span class="math inline">\hat{f}</span> would change if we had used a different training data set. High variance means that a small change in the training data leads to a large change in <span class="math inline">\hat{f}</span>.</p>
<p>The book gives the following equation for the decomposition of the MSE into the variance of our estimator, the squared bias of our estimator and the error term.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">
\begin{aligned}
E[y_0 - \hat{f}(x_0)]^2 = V[\hat{f}(x_0)] + [B(\hat{f}(x_0))]^2 + V[\epsilon]
\end{aligned}
</span></p>
<p>What does this equation tell us about the expected MSE on the test set. First, notice that every quantity is non-negative. Second, notice that irreducible error is part of the equation and serves as the upper bound because even if we had an estimate that had no variance and no bias, we would still have <span class="math inline">V[\epsilon]</span> left over.</p>
<p>The Bias-Variance tradeoff tells us that good test set performance has low bias and low variance. The tradeoff is that we can always draw some curve to the data at hand that perfectly captures every data point, but if we take that method to a different set of data points we will have increased variance. Conversely, we can always make variance 0 by fitting a horizontal line through data, but this will be unlikely to capture the true fit of the function increasing bias.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<section id="classification-loss-functions" class="level4" data-number="2.8.1">
<h4 data-number="2.8.1" class="anchored" data-anchor-id="classification-loss-functions"><span class="header-section-number">2.8.1</span> Classification Loss Functions</h4>
<p>Accuracy in the classification setting is conceptually similar with appropriate changes for the fact that we are now interested in the <em>training error rate</em>, the proportion of mistakes if we apply our learned estimate to the training observations.</p>
<p><span class="math display">
\begin{aligned}
Err = \frac{1}{n}\sum_{i=1}^nI(y_i \neq \hat{y}_i)
\end{aligned}
</span></p>
<p>The lowest test error occurs via the <em>Bayes Classifier</em> a method that assigns each observation to the most likely class given predictor values.</p>
<p><span class="math display">
\begin{aligned}
BC = Pr(Y=j|X = x_0)
\end{aligned}
</span></p>
<p>and the overall error rate will be <span class="math inline">1 - E[\max_{\j} Pr(Y=j|X))</span>, analogous to irreducible error in the classification case.</p>
</section>
</section>
<section id="lab" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="lab"><span class="header-section-number">2.9</span> Lab</h3>
<p>The lab at the end of Chapter 2 just reproduces some basic R functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Make a vector </span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">6</span>,<span class="dv">2</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">3</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Element wise addition</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>x <span class="sc">+</span> y </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  2 10  5</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Make a matrix </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>), </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">nrow =</span> <span class="dv">2</span>, </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">ncol =</span> <span class="dv">2</span>, </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    1    2
[2,]    3    4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         [,1]     [,2]
[1,] 1.000000 1.414214
[2,] 1.732051 2.000000</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate some random normal realizations </span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">500</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="at">mean =</span> <span class="dv">50</span>, <span class="at">sd =</span> .<span class="dv">1</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x,y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.995881</code></pre>
</div>
</div>
</section>
<section id="indexing-data" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="indexing-data"><span class="header-section-number">2.10</span> Indexing Data</h3>
<p>I’m choosing to skip the graphics section because this class uses the tidyverse and because base R graphics look ugly to me without extensive customization.</p>
<p>Indexing is incredibly important in any programming. A key note in R is that it is a 1 based index instead of a 0 based index.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>A <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>, <span class="dv">4</span>,<span class="dv">4</span>, <span class="at">byrow =</span> T)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12
[4,]   13   14   15   16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>A[<span class="dv">2</span>,<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">## We can also get slices </span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>A[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    2    4
[2,]   10   12</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>A[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    2    3    4
[2,]    6    7    8
[3,]   10   11   12</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>A[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,] <span class="co"># Getting just the first two rows </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>A[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="co"># first two columns</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    1    2
[2,]    5    6
[3,]    9   10
[4,]   13   14</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>A[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>),] <span class="co"># get only 2 and 4th row</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3] [,4]
[1,]    5    6    7    8
[2,]   13   14   15   16</code></pre>
</div>
</div>
</section>
<section id="exercises" class="level3" data-number="2.11">
<h3 data-number="2.11" class="anchored" data-anchor-id="exercises"><span class="header-section-number">2.11</span> Exercises</h3>
<section id="problem-7" class="level4" data-number="2.11.1">
<h4 data-number="2.11.1" class="anchored" data-anchor-id="problem-7"><span class="header-section-number">2.11.1</span> Problem 7</h4>
<p>Here is an example of applying a function to a data set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>knn_data <span class="ot">=</span> <span class="fu">tibble</span>(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">id =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">x3 =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"red"</span>, <span class="st">"red"</span>,<span class="st">"green"</span>,<span class="st">"green"</span>,<span class="st">"red"</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>getEuclideanDistance <span class="ot">=</span> <span class="cf">function</span>(<span class="at">point =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), vec){</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>  dist <span class="ot">=</span> <span class="fu">sqrt</span>((vec[<span class="dv">1</span>]<span class="sc">-</span> point[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (vec[<span class="dv">2</span>]<span class="sc">-</span>point[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (vec[<span class="dv">3</span>]<span class="sc">-</span>point[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(dist)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>getClosestPoints <span class="ot">=</span> <span class="cf">function</span>(k){</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>knn_data <span class="sc">|&gt;</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>()<span class="sc">|&gt;</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">d =</span> <span class="fu">getEuclideanDistance</span>(<span class="at">vec =</span> <span class="fu">c</span>(x1, x2, x3)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 6
# Rowwise: 
     id    x1    x2    x3 y         d
  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;
1     1     0     3     0 red    3   
2     2     2     0     0 red    2   
3     3     0     1     3 red    3.16
4     4     0     1     2 green  2.24
5     5    -1     0     1 green  1.41
6     6     1     1     1 red    1.73</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="chapter-3-linear-regression" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="chapter-3-linear-regression"><span class="header-section-number">3</span> Chapter 3: Linear Regression</h2>
<p>Linear regression is a simple approach for supervised learning. We might use it to determine if a relationship exists between variables, the strength of the (linear) relationship, associations between multiple variables, or whether there are interaction effects.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>The relationship we seek to fit is $$Y \approx B_i\textbf{X}$$ where we have a series of input variables in a design matrix <span class="math inline">\textbf{X}</span> and want to predict an output <span class="math inline">Y</span>.</p>
<p>There are lots of ways to do this in R.</p>
<ol type="1">
<li>We can do it by hand</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="do">## We will use the advertising data </span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">as.matrix</span>(ads[, <span class="st">"sales"</span>])</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">as.matrix</span>(ads[, <span class="fu">c</span>(<span class="st">"TV"</span>)])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="do">## We add an intercept term </span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>intercept <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">nrow</span>(ads))</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(intercept, x))</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="do">## (X'X)^-1X'y is the OLS estimator for any number </span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="do">## of input variables X</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="do">## The RHS of this is the way to do matrix multiplication and </span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="do">## the inverse and transpose operations in R</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>betas <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>Y</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>betas</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                [,1]
intercept 7.03259355
          0.04753664</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="do">### Calculate residuals</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>resids <span class="ot">=</span> Y <span class="sc">-</span> X<span class="sc">%*%</span>betas </span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="do">### Get degrees of freedom </span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">nrow</span>(X) <span class="sc">-</span> <span class="fu">ncol</span>(X) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="do">### Calculate residual variance </span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>resids_v <span class="ot">=</span> <span class="fu">sum</span>(resids<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>df </span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="do">### Get covariance matrix </span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>beta_cov <span class="ot">=</span> resids_v <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Takes the square root of the diagonal of the covariance matrix</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="do">## to get standard errors </span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>beta_se <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(beta_cov))</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">data.frame</span>(betas, beta_se),<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          betas beta_se
intercept 7.033   0.459
          0.048   0.003</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>We can use the base default <code>lm()</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="do">## broom tidy is a way to get the output of lm into a data frame</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Note that you will need to install the package before using</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(<span class="fu">lm</span>(sales <span class="sc">~</span> TV, <span class="at">data =</span> ads))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)   7.03     0.458        15.4 1.41e-35
2 TV            0.0475   0.00269      17.7 1.47e-42</code></pre>
</div>
</div>
<p>Note that our calculation “by hand” matches the estimate and standard errors of the default function.</p>
<ol start="3" type="1">
<li>We can use (and should!) a function from a package that lets us have better standard error estimation.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="do">## fixest is a created package, so must be installed before using</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(fixest<span class="sc">::</span><span class="fu">feols</span>(sales <span class="sc">~</span> TV, <span class="at">data =</span> ads))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)   7.03     0.458        15.4 1.41e-35
2 TV            0.0475   0.00269      17.7 1.47e-42</code></pre>
</div>
</div>
<section id="the-math" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="the-math"><span class="header-section-number">3.1</span> The Math</h3>
<p>The optimization problem is to take a set of data points <span class="math inline">\{(x_1, y_1),…(x_n,y_n)\}</span> that represent <span class="math inline">n</span> observation pairs. In the model in the chapter the points are the TV advertising budget and the product sales. We want to fit a model that minimizes the least squares criterion, which is our loss function. This model amounts to minimizing the total error between our model fit and all of the data points.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> The errors are defined by the residuals, and the residual sum of squares RSS is <span class="math inline">(y_1 - \hat{\beta_0} - ...\hat{\beta_nx_n)^2 + ... (y_n - \hat{\beta_0}x_n)^2}</span>. In words, we make some guesses about our beta coefficients and then subtract the prediction <span class="math inline">\hat{\beta_i}x_i</span> from the actual value observed.</p>
<p>It turns out in the simple linear regression case, the minimizes for this equation are:</p>
<p><span class="math display">\begin{aligned}
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_1}\bar{x} \\
\hat{\beta_1} &amp;= \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
\end{aligned}</span></p>
<p>The numerator in the second estimate the covariance between the vectors and the denominator is the variance of the input variables. The values with bars over them represent the sample means.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="do">## In code </span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">=</span> <span class="fu">cov</span>(ads[[<span class="st">"TV"</span>]], ads[[<span class="st">"sales"</span>]]) <span class="sc">/</span> <span class="fu">var</span>(ads[[<span class="st">"TV"</span>]])</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">=</span> <span class="fu">mean</span>(ads[[<span class="st">"sales"</span>]], <span class="at">na.rm =</span> T) <span class="sc">-</span> beta1 <span class="sc">*</span> <span class="fu">mean</span>(ads[[<span class="st">"TV"</span>]], <span class="at">na.rm =</span> T)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(beta0, beta1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]
beta0 7.03259355
beta1 0.04753664</code></pre>
</div>
</div>
<p>To assess uncertainty we compute standard errors. The formulas associated with the two coefficients are:</p>
<p><span class="math display">\begin{aligned}
SE_{\hat{\beta_0}} &amp;= \sigma^2[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2}] \\
SE_{\hat{\beta_1}} &amp;= \frac{\sigma^2}{\sum_{i=1}^n(x_i -\bar{x})^2}
\end{aligned}</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="do">## in code </span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="do">## How far off are we? </span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>sigma_squared <span class="ot">=</span> <span class="fu">var</span>(ads[[<span class="st">"sales"</span>]]<span class="sc">-</span> beta0 <span class="sc">-</span> beta1<span class="sc">*</span>ads[[<span class="st">"TV"</span>]])</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="do">## For readability purposes</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>numerator <span class="ot">=</span> sigma_squared<span class="sc">*</span><span class="fu">mean</span>(ads[[<span class="st">"TV"</span>]]<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>denominator <span class="ot">=</span> <span class="fu">sum</span>((ads[[<span class="st">"TV"</span>]]<span class="sc">-</span><span class="fu">mean</span>(ads[[<span class="st">"TV"</span>]]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>var_beta0 <span class="ot">=</span> numerator <span class="sc">/</span> denominator </span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>se_beta0 <span class="ot">=</span> <span class="fu">sqrt</span>(var_beta0)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>var_beta1 <span class="ot">=</span> sigma_squared <span class="sc">/</span> denominator</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>se_beta1 <span class="ot">=</span> <span class="fu">sqrt</span>(var_beta1)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(se_beta0, se_beta1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                [,1]
se_beta0 0.456691132
se_beta1 0.002683838</code></pre>
</div>
</div>
<p>If we want a confidence interval, we take the coefficient and multiply it plus/minus some critical value times the standard error.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="do">## approximately 95% confidence intervals </span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>lower_bound <span class="ot">=</span> beta1 <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>se_beta1 </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>upper_bound <span class="ot">=</span> beta1 <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>se_beta1 </span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(lower_bound, upper_bound)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  [,1]
lower_bound 0.04216896
upper_bound 0.05290432</code></pre>
</div>
</div>
<p>The t-statistic is defined as the coefficient divided by the standard error</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="do">## We'll get a slightly different answer here due to floating point operations and the calculation of the generalized variance</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>beta1 <span class="sc">/</span> se_beta1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 17.71218</code></pre>
</div>
</div>
</section>
<section id="multiple-regression" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="multiple-regression"><span class="header-section-number">3.2</span> Multiple Regression</h3>
<p>Everything that we have done so far generalizes to the multiple regression case. This is good because if it did not we likely would have made a basic category error.</p>
</section>
<section id="a-word-on-interactions" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="a-word-on-interactions"><span class="header-section-number">3.3</span> A word on interactions</h3>
<p>If you have an interaction in your model, you always need to put the separate variables into the model (e.g.&nbsp;the main effects). To add interactions into a model, we simply multiply variables together. Here are two commons ways to do that in R.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(<span class="fu">lm</span>(sales <span class="sc">~</span> TV <span class="sc">+</span> radio <span class="sc">+</span> TV<span class="sc">*</span>radio, <span class="at">data =</span> ads))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)  6.75    0.248         27.2  1.54e-68
2 TV           0.0191  0.00150       12.7  2.36e-27
3 radio        0.0289  0.00891        3.24 1.40e- 3
4 TV:radio     0.00109 0.0000524     20.7  2.76e-51</code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(<span class="fu">lm</span>(sales <span class="sc">~</span> TV <span class="sc">+</span> radio <span class="sc">+</span> TV<span class="sc">:</span>radio, <span class="at">data =</span> ads))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)  6.75    0.248         27.2  1.54e-68
2 TV           0.0191  0.00150       12.7  2.36e-27
3 radio        0.0289  0.00891        3.24 1.40e- 3
4 TV:radio     0.00109 0.0000524     20.7  2.76e-51</code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Also works in fixest </span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(fixest<span class="sc">::</span><span class="fu">feols</span>(sales <span class="sc">~</span> TV <span class="sc">+</span> radio <span class="sc">+</span> TV<span class="sc">:</span>radio, <span class="at">data =</span> ads))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)  6.75    0.248         27.2  1.54e-68
2 TV           0.0191  0.00150       12.7  2.36e-27
3 radio        0.0289  0.00891        3.24 1.40e- 3
4 TV:radio     0.00109 0.0000524     20.7  2.76e-51</code></pre>
</div>
</div>
<p>Similarly, if we want to include powers, we simply add in the appropriate order.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>auto <span class="ot">=</span> <span class="fu">read_csv</span>(<span class="st">"https://www.statlearning.com/s/Auto.csv"</span>, <span class="at">na =</span> <span class="fu">c</span>(<span class="st">"?"</span>,<span class="cn">NA</span>)) <span class="sc">|&gt;</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>  <span class="do">## drop missing</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="do">## poly is more flexible, raw = T makes it equivalent to the </span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="do">## the next way to do it. poly by default only uses orthogonal</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="do">## terms. Raw terms are likely to have collinearity </span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, <span class="dv">2</span>, <span class="at">raw=</span>T), <span class="at">data =</span> auto))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 5
  term                          estimate std.error statistic   p.value
  &lt;chr&gt;                            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)                   56.9      1.80          31.6 1.74e-109
2 poly(horsepower, 2, raw = T)1 -0.466    0.0311       -15.0 2.29e- 40
3 poly(horsepower, 2, raw = T)2  0.00123  0.000122      10.1 2.20e- 21</code></pre>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="do">## alternatively </span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> auto))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 5
  term            estimate std.error statistic   p.value
  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)     56.9      1.80          31.6 1.74e-109
2 horsepower      -0.466    0.0311       -15.0 2.29e- 40
3 I(horsepower^2)  0.00123  0.000122      10.1 2.20e- 21</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-4-classification" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="chapter-4-classification"><span class="header-section-number">4</span> Chapter 4: Classification</h2>
<p>We keep the same setting as before. We have a set of training observations that we can use as a classifier. We want the classifier to perform well not only on the training data, but also on data it has not seen before.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="do">### replicating Figure 4 </span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>default <span class="ot">=</span> ISLR2<span class="sc">::</span>Default</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>default <span class="sc">|&gt;</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(balance, income, <span class="at">color =</span> default, <span class="at">shape =</span> default))<span class="sc">+</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()<span class="sc">+</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)<span class="sc">+</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Overall Default Rates"</span>,</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Balance"</span>, <span class="at">y =</span> <span class="st">"Income"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Box plots </span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>default <span class="sc">|&gt;</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(default, balance))<span class="sc">+</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>()<span class="sc">+</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-21-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>default <span class="sc">|&gt;</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(default, income))<span class="sc">+</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>()<span class="sc">+</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-21-3.png" class="img-fluid" width="672"></p>
</div>
</div>
<section id="why-not-linear-regression" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="why-not-linear-regression"><span class="header-section-number">4.1</span> Why Not Linear Regression?</h3>
<p>In cases of qualitative response with multiple non-ordinal quantities, the coding of cases will affect the output in ways that we do not want it do. Since there is no natural way to convert a qualitative response with more than two levels, we shouldn’t use linear regression in this case.</p>
<p>In the binary case there’s really no problem, though the book mentions that you can get predictions greater than 1 or less than 0. Whether this is actually a problem is an open question since you can always just truncate and the linear regression approach will be the same as for linear discriminant analysis approaches.</p>
</section>
<section id="logistic-regression" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">4.2</span> Logistic Regression</h3>
<p>A logistic regression models the probability that Y belongs to a particular category. A logistic regression is so called because we model the probability with the logistic function.</p>
<p><span class="math display">
p(X) = \frac{e^{\textbf{X}\beta }}{1 + e^{\textbf{X}B}}
</span> We fit this model via maximum likelihood. We tend to use the odds for interpretation.</p>
<p><span class="math display">
\frac{p(X)}{1 - p(X)} = e^{\textbf{X}\beta}
</span> The odds can take on any value between 0 and infinity. Taking the logs of both sides reduces that equation to</p>
<p><span class="math display">
log \left(\frac{p(X)}{1 - p(X)}\right)= \textbf{X}\beta
</span> The interpretation of a <span class="math inline">\beta</span> coefficient is as follows. Increasing X_i by one unit changes the log odds by <span class="math inline">\beta_i</span>. Because the relationship between <span class="math inline">p(X)</span> and <span class="math inline">X</span> is not a straight line, it is not correct to interpret <span class="math inline">\beta_i</span> as a marginal change in <span class="math inline">p(X)</span> associated with a one-unit increase in <span class="math inline">X_i</span>.</p>
</section>
<section id="estimating-a-logistic-regression" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="estimating-a-logistic-regression"><span class="header-section-number">4.3</span> Estimating a logistic regression</h3>
<p>We use maximum likelihood. We’ll consider the simple binary case where we have coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> to estimate.</p>
<p><span class="math display">
l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i)\prod_{i':y_{i'}= 0} (1- p(x_i))
</span></p>
<p>In R:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>logit <span class="ot">=</span> <span class="fu">glm</span>(default<span class="sc">~</span>balance, <span class="at">data =</span> default, <span class="at">family =</span> <span class="st">"binomial"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-10.6513306</td>
<td style="text-align: right;">0.3611574</td>
<td style="text-align: right;">-29.49221</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">balance</td>
<td style="text-align: right;">0.0054989</td>
<td style="text-align: right;">0.0002204</td>
<td style="text-align: right;">24.95309</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>To get the log-odds</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="do">## We're passing in the default value of the intercept</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="do">## and a balance of $1000 to match the predictions example</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>betas <span class="ot">=</span> logit<span class="sc">$</span>coefficients <span class="sc">%*%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>odds <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>betas))</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>odds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]
[1,] 0.005752145</code></pre>
</div>
</div>
<p>Let’s try another example</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>l2 <span class="ot">=</span> <span class="fu">glm</span>(default <span class="sc">~</span> student, <span class="at">data =</span> default, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="do">## default for student </span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>betas <span class="ot">=</span> l2<span class="sc">$</span>coefficients <span class="sc">%*%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>yes <span class="ot">=</span> <span class="fu">exp</span>(betas)<span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(betas))</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>yes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,] 0.04313859</code></pre>
</div>
</div>
</section>
<section id="multiple-logistic-regression" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="multiple-logistic-regression"><span class="header-section-number">4.4</span> Multiple Logistic Regression</h3>
<p>The generalization works the same way as in the linear regression case.</p>
</section>
</section>
<section id="chapter-5-resampling-methods" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="chapter-5-resampling-methods"><span class="header-section-number">5</span> Chapter 5: Resampling Methods</h2>
<section id="sec-cv" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="sec-cv"><span class="header-section-number">5.1</span> Cross Validation</h3>
<p>Recall that there is a distinction between training error rate and test error rate. The <em>training error rate</em> is the average error that results from using a method on the training data. The <em>test error rate</em> the average error from using a learning method to predict the response on the new observation not from the training data.</p>
<section id="validation-set-approach" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="validation-set-approach"><span class="header-section-number">5.1.1</span> Validation Set Approach</h4>
<p>Suppose we want to estimate the test error associated with fitting a particular learning method on a set of observations. In the <em>Validation Set</em> approach, we randomly divide the available observations into a training set and a validation set. We fit the model on the training data, and the fitted model is used to predict the responses for the validation set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>auto <span class="ot">=</span> ISLR2<span class="sc">::</span>Auto</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Randomly split the 392 observations into two </span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="do">## sets of 196 observations </span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">392</span>, <span class="dv">196</span>, <span class="at">replace =</span> F)</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> auto[idx,]</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> auto[<span class="sc">-</span>idx,]</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> train)</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(model, test)</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>mse_pred <span class="ot">=</span> <span class="fu">mean</span>((auto<span class="sc">$</span>mpg[<span class="sc">-</span>idx] <span class="sc">-</span> preds)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are some drawbacks with just using a single validation set.</p>
<ol type="1">
<li>Validation estimates of the test error rate can be highly variable, depending on what observations are included.</li>
<li>Since only a subset of observations are used to fit the model, validation set error rate may tend to overestimate the test error rate for the model fit on the entire dataset.</li>
</ol>
<p>Cross validation addresses these two issues.</p>
</section>
<section id="leave-one-out-cross-validation-loocv" class="level4" data-number="5.1.2">
<h4 data-number="5.1.2" class="anchored" data-anchor-id="leave-one-out-cross-validation-loocv"><span class="header-section-number">5.1.2</span> Leave one out Cross Validation (LOOCV)</h4>
<p>We split the data into two parts. Instead of creating just two subsets of comparable size, we use a single observation <span class="math inline">(x_1, y_1)</span> for the validation set, and the remaining observations <span class="math inline">\{(x_2,y_2),…(x_n, y_n)\}</span> to make up the training set. We fit our learning method to the <span class="math inline">n-1</span> training observations and we make a prediction <span class="math inline">\hat{y}_1</span> to the excluded observation using its value of <span class="math inline">x_1</span>.</p>
<p>Because we did not use the observation in the fitting process, the MSE is approximately unbiased for the test error. However, this method by itself is highly variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="do">## A very simple example</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> .<span class="dv">5</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">x =</span> x)</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>train_mse <span class="ot">=</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"logical"</span>, <span class="at">length =</span> <span class="fu">nrow</span>(dat))</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>test_mse <span class="ot">=</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"logical"</span>, <span class="at">length =</span> <span class="fu">nrow</span>(dat))</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(dat)){</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>  out <span class="ot">=</span> dat[i,]</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(out)</span></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>  loo <span class="ot">=</span> dat[<span class="sc">-</span>i,]</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">=</span> <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> loo)</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>  train_mse[i] <span class="ot">=</span> <span class="fu">mean</span>((fit<span class="sc">$</span>model<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit)))</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>  test_mse[i] <span class="ot">=</span> <span class="fu">mean</span>((out[[<span class="st">"y"</span>]]<span class="sc">-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> out))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>(train_mse),<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(test_mse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9609283</code></pre>
</div>
</div>
<p>LOOCV has the potential to be expensive to implement because we have to fit our model <span class="math inline">n</span> times. With least squares linear or polynomial regression, it turns out that a shortcut makes the cost of LOOCV the same as a single model fit.</p>
<p><span class="math display">
CV_{(n)} = \frac{1}{n}\sum_{i=1}^n\left(\frac{y_i - \hat{y}_i}{1-h_i} \right)^2
</span> Here <span class="math inline">\hat{y}_i</span> is the <span class="math inline">ith</span> fitted value from the original least squares fit, and <span class="math inline">h_i</span> is the leverage defined by</p>
<p><span class="math display">
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i' = 1}^n (x_{i'} - \bar{x})^2}
</span> The shortcut equation is almost the same as the usual MSE except that the <span class="math inline">ith</span> residual is divided by <span class="math inline">1-h_i</span>. Leverage (<span class="math inline">h_i</span>) is between <span class="math inline">\frac{1}{n}</span> and 1. It reflects the amount that an observation influences its own fit. High leverage points’ residuals are inflated in the formula for the equality to hold.</p>
</section>
</section>
<section id="k-fold-cross-validation" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="k-fold-cross-validation"><span class="header-section-number">5.2</span> k-Fold Cross Validation</h3>
<p>Since LOOCV is expensive, an alternative is k-fold CV. The approach involves randomly dividing our set of observations into <span class="math inline">k</span> folds of approximately equal size. The first fold is the validation set and the learning method is fit on to the remaining <span class="math inline">k-1</span> folds. MSE is then computed on the observations in the hold out fold. We repeat this procedure k times for each different group and then calculate our estimate by taking the average of the values</p>
<p><span class="math display">
CV_{(k)} = \frac{1}{k}\sum_{i=1}^kMSE_i
</span> Perhaps unsurprisingly, LOOCV is a k-fold cross validation where k is the same as n.</p>
<section id="bias-variance-trade-off-for-k-fold-cross-validation" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="bias-variance-trade-off-for-k-fold-cross-validation"><span class="header-section-number">5.2.1</span> Bias-Variance Trade-off for k-fold Cross Validation</h4>
<p>k-fold CV often does better than LOOCV on test error because of the bias variance trade-off. While LOOCV gives approximately unbiased estimates of the test error, because we are fitting on a single unit each time the procedure will likely have high variance. Conversely, while k-fold cross validation contains an intermediate amount of bias (each training set contains $ $ observations) whenever <span class="math inline">k &lt; n</span> the LOOCV procedure will have higher variance than k-fold CV.</p>
<p>Typically we set k-fold to 5 or 10 which as rules of thumbs tend to yield test error estimates that suffer from neither excessively high bias nor from high variance.</p>
</section>
</section>
<section id="cross-validation-for-classification" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="cross-validation-for-classification"><span class="header-section-number">5.3</span> Cross Validation for Classification</h3>
<p>Same general ideas hold, except we use accuracy error.</p>
</section>
</section>
<section id="chapter-6-ridge-regression-and-lasso" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="chapter-6-ridge-regression-and-lasso"><span class="header-section-number">6</span> Chapter 6: Ridge Regression and Lasso</h2>
<section id="ridge-regression" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="ridge-regression"><span class="header-section-number">6.1</span> Ridge Regression</h3>
<p>Recall that the least squares fitting procedure finds appropriate weights <span class="math inline">\beta_0,…,\beta_n</span> by minimizing the residual sum of squares (RSS)</p>
<p><span class="math display">
RSS = \sum_{i=1}^n\left(y_i -\beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2
</span></p>
<p>Ridge regression uses a different loss function. Ridge regression finds coefficients <span class="math inline">\hat{\beta}^R</span> that minimizes:</p>
<p><span class="math display">
\sum_{i=1}^n\left(y_i -\beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda \sum_{j=1}^p\beta_j^2
</span> Here <span class="math inline">\lambda</span> is a tuning parameter. Staring at the equation we see that ridge regression trades off two different criteria. We want coefficient estimates that will fit the data well, but the second term is a shrinkage penalty. It is small whenever the <span class="math inline">\beta_i</span> are close to 0 and so has the effect of shrinking the estimates towards 0. When <span class="math inline">\lambda=0</span> there is no effect and it is the same estimator as linear regression. As <span class="math inline">\lambda \rightarrow \infty</span>, the impact of the penalty grows so the ridge regression coefficient estimates will approach 0. The shrinkage penalty is not applied to the intercept.</p>
<p>We often center the predictors to have mean zero before ridge regression is performed. In this case, the estimated intercept is:</p>
<p><span class="math display">
\hat{\beta_0} = \bar{y}
</span> We do this because ridge regression coefficients are not scale equivalent. Ridge regression coefficients depend not only on the value of <span class="math inline">\lambda</span> but also on the scaling of the <span class="math inline">jth</span> predictor. It may also depend on the values of other predictors. We scale via the formula:</p>
<p><span class="math display">
\tilde{x}_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij} - \bar{x}_j)^2}}
</span> The denominator is the estimated standard deviation of the <span class="math inline">jth</span> predictor so all standardized predictors will have a standard deviation of 1. An implication is that the final fit will not depend on the scale of the predictors.</p>
<section id="application" class="level5" data-number="6.1.0.1">
<h5 data-number="6.1.0.1" class="anchored" data-anchor-id="application"><span class="header-section-number">6.1.0.1</span> Application</h5>
<p>TODO: Insert these in</p>
<div class="cell" data-hash="ISLR_notes_cache/html/unnamed-chunk-28_1636be007bf5d0df8d1e4bf0ae4f6c5a">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">=</span> <span class="fu">na.omit</span>(ISLR2<span class="sc">::</span>Hitters)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="do">##. standardize all predictors </span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>center <span class="ot">=</span> <span class="cf">function</span>(x){</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Function to center a vector around</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>  <span class="do">## a mean of 0 </span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(x <span class="sc">-</span> <span class="fu">mean</span>(x))</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>standardize <span class="ot">=</span> <span class="cf">function</span>(x){</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>  centered <span class="ot">=</span> <span class="fu">center</span>(x)</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>  <span class="do">## To match R scale() implementation</span></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>  <span class="do">## divide by n-1 </span></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>  var <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">center</span>(x)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(x)<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>  sd <span class="ot">=</span> <span class="fu">sqrt</span>(var)</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">center</span>(x)<span class="sc">/</span>sd)</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., Hitters)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> Hitters[[<span class="st">"Salary"</span>]]</span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Set alpha to 0 to get ridge. Set alpha to 1 to get lasso</span></span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>ridge.mod <span class="ot">=</span> <span class="fu">glmnet</span>(x,y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid)</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge.mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>lasso.mod <span class="ot">=</span> <span class="fu">glmnet</span>(x,y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):
collapsing to unique 'x' values</code></pre>
</div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-28-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Using cross-validation to find <span class="math inline">\lambda</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x), <span class="fu">nrow</span>(x)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> (<span class="sc">-</span>train)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>y.test <span class="ot">=</span> y[test]</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="do">### Ridge </span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">=</span> <span class="fu">cv.glmnet</span>(x[train,], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">=</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>bestlam</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 326.0828</code></pre>
</div>
</div>
<p>Now do the same with the Lasso</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="do">### Lasso </span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>lasso.mod <span class="ot">=</span> <span class="fu">glmnet</span>(x[train,], y[train], <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):
collapsing to unique 'x' values</code></pre>
</div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">=</span> <span class="fu">cv.glmnet</span>(x[train,],y[train], <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="ISLR_notes_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">=</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>lasso.pred <span class="ot">=</span> <span class="fu">predict</span>(lasso.mod, <span class="at">s =</span> bestlam, <span class="at">newx =</span> x[test,])</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((lasso.pred<span class="sc">-</span>y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 143673.6</code></pre>
</div>
</div>
</section>
<section id="why-does-ridge-regression-improve-over-ols" class="level4" data-number="6.1.1">
<h4 data-number="6.1.1" class="anchored" data-anchor-id="why-does-ridge-regression-improve-over-ols"><span class="header-section-number">6.1.1</span> Why does Ridge Regression improve over OLS</h4>
<p>Ridge regression’s advantage is in the bias-variance tradeoff. As <span class="math inline">\lambda</span> increases the flexibility of the ridge regression fit decreases.</p>
<ul>
<li>The variance decreases and the bias of the estimate increases.</li>
</ul>
<p>Because the test MSE is composed of both bias and variance, a substantial decrease in the variance will improve predictions on this measure of success.</p>
<p>In general, when the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. A small change in the training data can cause a large change in the coefficient estimates. Ridge regression works best in situations where the OLS estimate is high variance.</p>
<p>The primary disadvantage for ridge regression is that it includes all <span class="math inline">p</span> predictors in the final model, which can create a challenge for model interpretation when <span class="math inline">p</span> is large.</p>
</section>
</section>
<section id="the-lasso" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="the-lasso"><span class="header-section-number">6.2</span> The Lasso</h3>
<p>Unlike ridge regression the lasso minimizes the following quantity:</p>
<p><span class="math display">
\sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij})^2 + \lambda \sum_{j=1}^p|\beta_j|
</span> The difference between ridge and lasso is the penalty term. In the lasso, we use the L1 norm instead of the L2 norm. In the lasso, the penalty has the effect of forcing some coefficient estimates to be exactly equal to zero when the tuning parameter is sufficiently large. A consequence is that the lasso is a variable selection model that yields sparse models. Depending on the value of <span class="math inline">\lambda</span> the lasso can produce a model involving any number of variables, while ridge regression will always include all predictors in the model even though the magnitude of the coefficient estimates will depend on <span class="math inline">\lambda</span>.</p>
</section>
<section id="comparing-lasso-and-ridge-regression" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="comparing-lasso-and-ridge-regression"><span class="header-section-number">6.3</span> Comparing lasso and ridge regression</h3>
<p>Both the lasso and ridge regression will often lead to qualitatively similar behavior in that as <span class="math inline">\lambda \rightarrow \infty</span> the variance decreases and the bias increases. In general neither ridge or lasso will universally dominate the other.</p>
<ul>
<li><p>The lasso will often perform better when there are relatively small number of predictors that have substantial coefficients and the remaining predictors have coefficients close to 0. Lasso like ridge yields a decrease in variance at the expense of a increase in bias when least squares has high variance.</p></li>
<li><p>Ridge will perform better when the response is a function of many predictors with similar coefficient sizes.</p></li>
</ul>
<p>Cross validation can be used to determine which approach is better. Generally speaking, ridge regression more or less shrinks every dimension by the same proportion whereas the lasso more or less shrinks all coefficients towards zero by a similar amount and sufficiently small coefficients go to 0.</p>
</section>
<section id="selecting-the-tuning-parameter" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="selecting-the-tuning-parameter"><span class="header-section-number">6.4</span> Selecting the tuning parameter</h3>
<p>Use cross validation on the penalty term. Create a grid of <span class="math inline">\lambda</span> values and compute the cross validation error for each as described in the <a href="#sec-cv">Section&nbsp;5.1</a> section.</p>
</section>
</section>
<section id="chapter-8-tree-based-methods" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="chapter-8-tree-based-methods"><span class="header-section-number">7</span> Chapter 8: Tree Based Methods</h2>
<section id="cart" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="cart"><span class="header-section-number">7.1</span> CART</h3>
<p>Building a regression tree proceeds loosely in two steps.</p>
<ol type="1">
<li>Divide the predictor space into J distinct and non-overlapping regions <span class="math inline">R_1, …, R_j</span>.</li>
<li>For every observation that falls into the region <span class="math inline">R_j</span> make the same prediction which is simply the mean of the response values for the training observation in <span class="math inline">R_j</span>.</li>
</ol>
<p>To construct regions we divide the predictor space into boxes, the goal of which is to find boxes that minimize the RSS <span class="math inline">\sum_{j=1}^J\sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2</span></p>
<p>It is computational infeasible to consider every possible partition of the feature space, so we use a top down greedy approach called recursive binary splitting. This begins at the top of the tree and at each step the best split is made at that particular step instead of looking ahead to a possible better split at a future node.</p>
<p>For recursive binary splitting, we first select the predictor and the cutpoint such that splitting the predictor space leads to the greatest possible reduction in RSS. We repeat this process for all of our regions until a stopping criterion is reached.</p>
<p>We prune trees via cost complexity pruning. Instead of considering every possible subtree, we consider the sequence of trees indexed by a non-negative tuning parameter <span class="math inline">\alpha</span>. The general algorithm is laid out on page 333.</p>
<p>Classification trees are made via a similar process except that we use it to predict a qualitative rather than quantitative response. In addition, we cannot use RSS as our criterion for binary splits, so we use the binary entropy or Gini indexing instead.</p>
</section>
<section id="bagging" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="bagging"><span class="header-section-number">7.2</span> Bagging</h3>
<p>Decision trees can suffer from high variance. Bootstrap aggregation (bagging) is a general purpose procedure fro reducing the variance of a statistical learning methods.We generate B different bootstrapped training data sets, train our method on the <span class="math inline">b^{th}</span> bootstrapped training set in order to get <span class="math inline">\hat{f}^{*b}(x)</span> and then average over all predictions to obtain <span class="math inline">\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}B\hat{f}^{*b}(x)</span></p>
<p>Bagging is particularly useful with decision trees. We construct B regression trees using B boostrapped training sets and average the resulting predictions. Each tree has high variance and low bias, and averaging over all trees reduces variances. For qualitative trees we take a majority vote for the average class prediction.</p>
<section id="out-of-bag-error-estimation" class="level4" data-number="7.2.1">
<h4 data-number="7.2.1" class="anchored" data-anchor-id="out-of-bag-error-estimation"><span class="header-section-number">7.2.1</span> Out of Bag Error Estimation</h4>
<p>On average, each bagged tree makes use of about 2/3 of the observations. The remaining 1/3 of the observations not used to fit a given bagged tree are referred to as out of bag (OOB) observations. We can use this to calculate the overall OOB MSE or classification error, which is a valid estimate of the test error for the bagged model because the response for each observation is predicted using only the trees not fit using that observation.</p>
</section>
</section>
<section id="boosting" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="boosting"><span class="header-section-number">7.3</span> Boosting</h3>
<p>In boosting, we grow trees sequentially using information from previously grown trees. We do not bootstrap sample; instead each tree is fit on a modified version of the original dataset.</p>
<p>Unlike fitting a large decision tree, the boosting approach learns slowly. Given the current model, we fit a decision tree to the residuals from teh model. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes determined by the max node size parameter n the algorithm. By fitting small trees to the residuals, we slowly improve our model fit in areas where it does not perform well. #### Algorithm</p>
<ol type="1">
<li><p>Set <span class="math inline">\hat{f}(x) = 0, r_i = y_i</span> for all <span class="math inline">i</span> in the training set.</p></li>
<li><p>For b = 1,2,…, B repeat:</p></li>
</ol>
<ul>
<li>Fit a tree with d splits (d + 1 terminal nodes) to the training data (X,r).</li>
<li>Update model by adding in a shrunken version of the new tree <span class="math inline">\hat{f}(x) = \hat{f}(x) + \lambda \hat{f}^b(x)</span></li>
<li>Update the residuals <span class="math inline">r_i = r_i + \lambda \hat{f}^b(x)</span></li>
</ul>
<ol start="3" type="1">
<li>Output the boosted model. <span class="math inline">\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)</span></li>
</ol>
<p>We have three parameters. The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B.</p>
<p>The shrinkage parameter <span class="math inline">\lambda</span>, a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 and 0.001 and the right choice can depend on the problem. Very small <span class="math inline">\lambda</span> using a very large value of B in order to achieve good performance.</p>
<p>The interaction depth/number <span class="math inline">d</span> of splits in each tree, which controls the complexity of the boosted ensemble. Often <span class="math inline">d = 1</span> works well, in which case each tree is a stump, consisting of a single split. It controls the interaction order of the boosted model because d splits can involve at most d variables.</p>
</section>
<section id="random-forests" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">7.4</span> Random Forests</h3>
<p>Random forests provide an improvement over bagged trees by decorrelating the trees. We build a number of decision trees on bootstrapped random samples. When building the tree, each time a split is considered we take a random sample of m predictors to be split candidates from the full set of p predictors. We typically choose <span class="math inline">m \approx \sqrt{p}</span>. The main difference between Random Forests and bagging is the size of the predictor subset. When <span class="math inline">m = p</span> random forests are equivalent to bagging.</p>
</section>
<section id="bart" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="bart"><span class="header-section-number">7.5</span> BART</h3>
<p>Bayesian Additive Regression Trees (BART) are another ensemble method. In BART we have <span class="math inline">K</span> number of regression trees and <span class="math inline">B</span> number of iterations for which the algorithm runs. <span class="math inline">\hat{f}^B_k(x)</span> represents the prediction at <span class="math inline">x</span> for the <span class="math inline">k^{th}</span> regression tree at the <span class="math inline">b^{th}</span> iteration. At the end of each iteration we sum all K trees.</p>
<p>We typically throw away the first few BART prediction models as they are part of the burn-in period.</p>
<section id="algorithm" class="level4" data-number="7.5.1">
<h4 data-number="7.5.1" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">7.5.1</span> Algorithm</h4>
<ol type="1">
<li><p>Let <span class="math inline">\hat{f}^1_1(x) = ... = \hat{f}^1_K(x) = \frac{1}{nK}\sum_{i=1}^ny_i</span></p></li>
<li><p>Compute <span class="math inline">\hat{f}^1(x) = \sum_{k=1}^K \hat{f}^1_k(x) = \frac{1}{n}\sum_{i=1}^n y_i</span></p></li>
<li><p>For b = 2, … B:</p></li>
</ol>
<ul>
<li>For k = 1,2,…,K:
<ul>
<li><p>For i = 1,…, n, compute the current partial residual <span class="math inline">r_i = y_i - \sum_{k' &lt; k}\hat{f}^b_{k'}(x_i) - \sum_{k' &gt; k}\hat{f}^{b-1}_{k'}(x_i)</span></p></li>
<li><p>Fit a new tree to <span class="math inline">r_i</span> by randomly perturbing the <span class="math inline">k^{th}</span> tree from the previous iteration. Perturbations that improve the fit are favored.</p></li>
</ul></li>
<li>Compute <span class="math inline">\hat{f}^b(x) = \sum_{k=1}^k \hat{f}^b_k(x)</span></li>
</ul>
<ol start="4" type="1">
<li>Compute the mean after L burn-in samples. <span class="math inline">\hat{f}(x) = \frac{1}{B-L}\sum_{b=L+1}^B\hat{f}%b(x)</span></li>
</ol>
</section>
</section>
</section>
<section id="chapter-12-unsupervised-learning" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="chapter-12-unsupervised-learning"><span class="header-section-number">8</span> Chapter 12: Unsupervised Learning</h2>
<section id="principal-components-analysis" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="principal-components-analysis"><span class="header-section-number">8.1</span> Principal Components Analysis</h3>
<section id="proportion-of-variance-explained" class="level4" data-number="8.1.1">
<h4 data-number="8.1.1" class="anchored" data-anchor-id="proportion-of-variance-explained"><span class="header-section-number">8.1.1</span> Proportion of Variance Explained</h4>
</section>
<section id="scaling-variables" class="level4" data-number="8.1.2">
<h4 data-number="8.1.2" class="anchored" data-anchor-id="scaling-variables"><span class="header-section-number">8.1.2</span> Scaling Variables</h4>
</section>
<section id="deciding-how-many-principal-components-to-use" class="level4" data-number="8.1.3">
<h4 data-number="8.1.3" class="anchored" data-anchor-id="deciding-how-many-principal-components-to-use"><span class="header-section-number">8.1.3</span> Deciding how many Principal Components to Use</h4>
</section>
</section>
<section id="clustering-methods" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="clustering-methods"><span class="header-section-number">8.2</span> Clustering Methods</h3>
<section id="k-means-clustering" class="level4" data-number="8.2.1">
<h4 data-number="8.2.1" class="anchored" data-anchor-id="k-means-clustering"><span class="header-section-number">8.2.1</span> K-Means Clustering</h4>
</section>
<section id="hierarchical-clustering" class="level4" data-number="8.2.2">
<h4 data-number="8.2.2" class="anchored" data-anchor-id="hierarchical-clustering"><span class="header-section-number">8.2.2</span> Hierarchical Clustering</h4>
</section>
<section id="choice-of-dissimilarity-measure" class="level4" data-number="8.2.3">
<h4 data-number="8.2.3" class="anchored" data-anchor-id="choice-of-dissimilarity-measure"><span class="header-section-number">8.2.3</span> Choice of Dissimilarity Measure</h4>
</section>
<section id="practical-issues-in-clustering" class="level4" data-number="8.2.4">
<h4 data-number="8.2.4" class="anchored" data-anchor-id="practical-issues-in-clustering"><span class="header-section-number">8.2.4</span> Practical Issues in Clustering</h4>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Strictly speaking, input variables are arguments to the fixed but unknown function <em>f</em> that we are trying to learn.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Whether non-causal inferences are interesting is a separate question.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>This distinction is annoying to someone who comes at these questions from a different discipline. Much of the machine learning literature’s hodgepodge of terms tells you more about the authors’ educational background as it does the problem at hand.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The derivation is boring. Recall the decomposition of error and add and subtract <span class="math inline">E[\hat{f}]</span> from the right hand side. Group the terms together and substitute in appropriate named quantitites.</p>
<p><span class="math display">
\begin{aligned}
E[Y-\hat{Y}]^2 &amp;= E[(f- \hat{f})^2] + V[\epsilon] \\
E[Y-\hat{Y}]^2 &amp;= E[f + E[\hat{f}] - E[\hat{f}] - \hat{f}] + V[\epsilon] \\
E[Y-\hat{Y}]^2 &amp;= E[\hat{f} - E[\hat{f}]]^2 + [f - E[\hat{f}]]^2 + V[\epsilon] \\
E[Y-\hat{Y}]^2 &amp;= V[\hat{f}] + B[\hat{f}]^2 + V[\epsilon]
\end{aligned}
</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>In causal inference, we tend to care a lot more about the bias of our estimator than the variance.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The book often refers to this as “synergy”<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Note that this sounds an awful lot like finding an average and that is intentional.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>